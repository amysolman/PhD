---
title: "Network Analysis"
author: "Amy Solman"
date: "10/11/2021"
output: html_document
---

This script will perform network analysis on my count data while exploring the differences between rare, abundant and intermediate subcommunities.

Step One: Clear workspace and load packages
Step Two: Import data
Step Three: Normalise data
Step Four: Classify ASVs by abundance
Step Five: Filter ASVs to simplify network analysis
Step Six: Perform correlation analysis and export correlation analysis as network
Step Seven: Calculate node-level properties (degree, betweenness centrality, closeness centrality, eignevector centrality)
Step Eight: Generate subcommunity networks
Step Nine: Calculate node-level properties
Step Ten: How many connections are there within and between the subcommunities?
Step Eleven: Test for staistical differences between node-level properties of rare, abundant and intermediate subcommunities
Step Twelve: Generate boxplots of node-level properties for subcommunities
Step Thirteen: Identify keystone ASVs
Step Fourteen: Generate random networks
Step Fifteen: Calculate node-level properties for random network
Step Sixteen: Calculate network-level properties for random network
Step Seventeen: Compare the network- and node-level properties of the real and random networks
Step Eighteen: Identify if the real network exhibits scale-free characteristics

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Step One: Clear workspace and load packages
```{r}
rm(list=ls())
graphics.off()

#library(phyloseq)
library(Hmisc)
library(igraph)
#library(agricolae)
library(fdrtool)
library(vegan) #rarecurve function
library(dplyr)
library(ggpubr)
library(car) #levene test
library(plyr)
#remotes::install_github("vmikk/metagMisc")
#library(metagMisc)
```

Step Two: Import data and subset by pole
```{r}
phylo <- readRDS("../data-output/18S-my-phyloseq-object.rds") # phyloseq object

#subset data by pole
arctic <- subset_samples(phylo, Pole=="Arctic")

#this step removed too many ASVs that network analysis was impossible. Therefore I'm not going to remove unassigned taxa and just focus on their abundance and not taxonomy.
#filter out unassigned ASVs
#arctic.filter <- subset_taxa(arctic, Phylum != "NA")
```


Step Three: Normalize data
```{r}
#draw rarefaction curves
rarecurve(t(otu_table(arctic)), step=50, cex=0.5)

#remove samples with less than 9000 reads (we need at least 10000 reads per sample)
arctic.prune = prune_samples(sample_sums(arctic)>=9000, arctic)

#if we re-plot our curve we can see the smallest sample was removed
rarecurve(t(otu_table(arctic.prune)), step=50, cex=0.5)

#rarefy data to minimum num sequences = 13578
ps.rarefied = rarefy_even_depth(arctic.prune, rngseed=1, sample.size=min(sample_sums(arctic.prune)), replace=F)
```

Step Four: Classify ASVs by abundance
```{r}
#transform our count data into relative abundances
rel_abun = transform_sample_counts(ps.rarefied, function(x) x / sum(x) )

#get our ASV table
asv_tab <- data.frame(t(otu_table(rel_abun)), check.names=FALSE)

#seperate ASVs by abundance
rare <- asv_tab[,colSums(asv_tab)/nrow(asv_tab)<0.0001] #less than 0.01%
abundant <- asv_tab[,colSums(asv_tab)/nrow(asv_tab)>0.001] #more than 0.1%
intermediate <- asv_tab[,colSums(asv_tab)/nrow(asv_tab)>=0.0001 & colSums(asv_tab)/nrow(asv_tab)<=0.001] #between 0.01 - 0.1%

```

Step Five: Filter ASVs to simplify network analysis
```{r}
#remove ASVs present in less than 10% samples
df <- 1*(asv_tab>0) #presence/abundance df
pres_vec <- vector() 
asv_vec <- vector()
for (k in 1:ncol(df)){  #for each ASV
  pres <- as.numeric(colSums(df)[k])/nrow(df) #find the percentage of samples the ASV is present in
  pres_vec <- c(pres_vec, pres)
  asv_vec <- c(asv_vec, colnames(df)[k])
}
df_2 <- data.frame(ASV=asv_vec, Present=pres_vec) #df of percentage of samples asvs are present in

trim_me <- subset(df_2, df_2$Present >= 0.10) #only keep ASVs in >= 10% of samples (that's at least two samples)

#use ASV IDs to subset count table
asv_pres_trim <- asv_tab[, (colnames(asv_tab) %in% trim_me$ASV)]

# #alternatively we could remove ASVs with less than 10 counts
# #get our ASV count table
# asv_counts <- data.frame(t(otu_table(ps.rarefied)))
# asv_counts_trim = asv_counts[,colSums(asv_counts) > 10]
```

Step Six: Perform correlation analysis
```{r}
#find the spearmans correlations between asvs
cor_analysis <-rcorr(as.matrix(asv_pres_trim),type="spearman") #I have used asv_pres_trim here because when I use asv_counts_trim there are no positive correlations

#asign the matrix of correlations and the p-values to r and p, respectively
cor_r<-cor_analysis$r
cor_p<-cor_analysis$P 

#matrix diagonals - apply the value 1 to the diagonal values of the p matrix (instead of NA as they were before)
diag(cor_p)<-1

#make p-values matrix into vector
cor_pp<-as.vector(cor_p)

#how many of our p values are < 0.01
length(cor_pp[cor_pp > 0 & cor_pp < 0.01]) #2070 over 20% of our correlations are significant

#estimate false discovery rates
cor_qvalue<-fdrtool(cor_pp,statistic="pvalue")

#extract the vector with q-values (density based falese discovery rates)
cor_q<-cor_qvalue$qval

#what are our unique adjusted p values?
unique(cor_q) #lots of them! Essentially we don't want to see 1 NaN here (this means we have no significant adjusted p values)

#create a matrix with the q-values with the same number of rows and columns as p-values matrix
cor_q<-matrix(cor_q,nrow(cor_p),ncol(cor_p))

#make any value of q > 0.01 a 0
cor_q[cor_q>0.01]<-0

#make any value <= 0.01 & greater than 0 = 1
cor_q[cor_q<=0.01&cor_q>0]<-1

#let's look at the range of correlation coefficients
hist(cor_r)

#are there any with coefficients > | 0.6|?
length(cor_r[cor_r < -0.6]) #there are 20 negative correlation coefficients
length(cor_r[cor_r > 0.6]) #there 2624 positive correlation coefficients

#change the value of any correlation coefficients < | 0.6 | to 0
cor_r[abs(cor_r)<0.6]<-0

#multiple the correlation coefficients by q-values so that insignificant coefficients become 0
cor<-cor_r*cor_q

#are there any with coefficients > | 0.6| after removing non-significant correlations?
length(cor[cor < -0.6]) #there are 0 negative correlation coefficients
length(cor[cor > 0.6]) #2036 positive correlations

#create igraph graph from adjacency matrix
cor_g <- graph.adjacency(cor, weighted=TRUE, mode="undirected")

#simplify the graph so it does not contain loops or multiple edges
cor_g <- simplify(cor_g)

#delete vertices from the graph
cor_g<-delete.vertices(cor_g,names(degree(cor_g)[degree(cor_g)==0]))

#export graph
write.graph(cor_g,"results/arctic_network.gml", format="gml")
```

Step Seven: Calculate node-level properties: degree, betweenness centrality, closeness centrality, eigenvector centrality
```{r}
#create a dataframe with 5 columns and a row for each node (ASV)
df<-as.data.frame(matrix(NA,ncol=5,nrow=length(degree(cor_g))))

#make ASV IDs row names of df
rownames(df)<-names(degree(cor_g))

#name the colums the node-level topological features
colnames(df)<-c("degree","betweenness","closeness","eigenvector","category")

#categorise the ASVs as rare, abundant or intermediate
df[intersect(names(degree(cor_g)),colnames(rare)),5]<-"rare"
df[intersect(names(degree(cor_g)),colnames(abundant)),5]<-"abundant"
df[intersect(names(degree(cor_g)),colnames(intermediate)),5]<-"intermediate"

#get betweenness
btw<-betweenness(cor_g)
#get closeness centrality
cls<-closeness(cor_g)
#get eigenvector centrality
egv<-evcent(cor_g)

#put the topological features into the dataframe
df[,1]<-degree(cor_g)
df[,2]<-btw
df[,3]<-cls
df[,4]<-egv$vector
```

Step Eight: Generate subcommunity networks
```{r}
#subset the node level dataframe by rare ASVs
a<-subset(df,category=="rare")
g_ra<-induced_subgraph(cor_g,rownames(a))
#abundant
a<-subset(df,category=="abundant")
g_abd<-induced_subgraph(cor_g,rownames(a))
#intermediate
a<-subset(df,category=="intermediate")
g_int<-induced_subgraph(cor_g,rownames(a))
```

Step Nine: Calculate network-level properties
```{r}
#number of nodes (a.k.a. vertices/ASVs)
gorder(cor_g) #total = 45
gorder(g_ra) #rare = 8
gorder(g_abd) #abundant = 8
gorder(g_int) #intermediate = 29

#number of edges
gsize(cor_g) #total = 505
gsize(g_ra) #rare = 2
gsize(g_abd) #abundant = 11
gsize(g_int) #intermediate = 221


#mean node degree (number of edges), clustering coefficient (probability that the adjacent vertices of a vertex are connected), average path length, modularity, density, network diameter 
#create a dataframe to store out network features
net_df<-as.data.frame(matrix(NA,ncol=6,nrow=4))

net_df[1,]<-c(mean(degree(cor_g)),transitivity(cor_g),average.path.length(cor_g),
graph.density(cor_g),diameter(cor_g),modularity(walktrap.community(cor_g)))

net_df[2,]<-c(mean(degree(g_abd)),transitivity(g_abd),average.path.length(g_abd),
graph.density(g_abd),diameter(g_abd),modularity(walktrap.community(g_abd)))

net_df[3,]<-c(mean(degree(g_ra)),transitivity(g_ra),average.path.length(g_ra),
graph.density(g_ra),diameter(g_ra),modularity(walktrap.community(g_ra)))

net_df[4,]<-c(mean(degree(g_int)),transitivity(g_int),average.path.length(g_int),
graph.density(g_int),diameter(g_int),modularity(walktrap.community(g_int)))

colnames(net_df)<-c("AveDegree","ClustCoef","AvePathLen","Density","Diameter","Modularity")
rownames(net_df)<-c("Total", "Abundant","Rare","Intermediate")
```

Step Ten: How many connections are there between the subcommunities?
```{r}
#extract the adjacency matric from the simplified igraph
mat <- as.data.frame(as_adjacency_matrix(cor_g, sparse = FALSE))

x <- tidyr::gather(mat) #gather data 
x$ASV_Two <- rep(colnames(mat), nrow(mat)) #add second asv
x[x==0] <- NA #set 0 to NA
x2<-x[complete.cases(x),]#remove rows with NA
x2 <- subset(x2, select = -c(value)) #remove value column
colnames(x2) <- c("ASV_One", "ASV_Two") #rename columns

#remove rows that have the repeated pairs of ASVs
dat <- data.frame(
  var1 = x2$ASV_One,
  var2 = x2$ASV_Two,
  cor = rep(1, nrow(x2)))
dat1 <- dat %>% 
  filter(!duplicated(paste0(pmax(var1, var2), pmin(var1, var2))))

#remove cor column
dat1 <- dat1[,-c(3)]

#get abundance of ASV One
for (i in 1:nrow(dat1)){
  if(dat1$var1[i] %in% colnames(rare)){
    dat1$var3[i] <- "rare"
  } else if(dat1$var1[i] %in% colnames(abundant)){
    dat1$var3[i] <- "abundant"
  } else {
    dat1$var3[i] <- "intermediate"
  }
}

#get abundance of ASV Two
for (i in 1:nrow(dat1)){
  if(dat1$var2[i] %in% colnames(rare)){
    dat1$var4[i] <- "rare"
  } else if(dat1$var2[i] %in% colnames(abundant)){
    dat1$var4[i] <- "abundant"
  } else {
    dat1$var4[i] <- "intermediate"
  }
}

#rename the columns
colnames(dat1) <- c("ASV_One", "ASV_Two", "Abund_One", "Abund_Two")

#table the data
table(dat1[,3:4])
#from this we can see we have:
# 11 abundant - abundant correlations
# 69+83 = 152 abundant - intermediate correlations
# 26+11 = 37 abundant - rare correlations
# 221 intermediate - intermediate correlations
# 49+33=82 intermediate - rare correlations
# 2 rare - rare correlations
```

Step Eleven: Test for staistical differences between node-level properties of rare, abundant and intermediate subcommunities
```{r}
#if there a significant differences in the degree of abundant, intermediate and rare asvs?
#are our data normally distributed?

#compute summary stats
df$category <- as.factor(df$category)
levels(df$category)

#library(dplyr)
group_by(df, category) %>%
  dplyr::summarise( #dplyr:: here because we've loaded the plyr package and they have similar commands so we need to specify which one we want
    count = n(),
    mean = mean(degree, na.rm = TRUE),
    sd = sd(degree, na.rm = TRUE)
  )

#Visualise
ggboxplot(df, x = "category", y = "degree", 
          color = "category", palette = c("#00AFBB", "#E7B800", "#FC4E07"),
          order = c("abundant", "intermediate", "rare"),
          ylab = "Degree", xlab = "Category")

# Compute the analysis of variance
res.aov <- aov(degree ~ category, data = df)
# Summary of the analysis
summary(res.aov) #there is NO significant difference between groups

TukeyHSD(res.aov) #all the categories are NOT significantly different

#Test assumptions of validity
#1. do we have homogeny of variances?
plot(res.aov, 1)
#or we can use the levene test
leveneTest(degree ~ category, data = df) # p > 0.05 so no evidence to suggest variance across groups is statistically significantly different

#2. Are the data normally distributed?
plot(res.aov, 2) #points somehwat follow the line

#we can check normality with Shapiro-Wilk test
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals ) #p < 0.05 so normality assumption is violated

#We need the non-parametric alternative
kruskal.test(degree ~ category, data = df) #p > 0.05 so categories are NOT significantly different
kruskal.test(betweenness ~ category, data = df) #p < 0.05 so categories are significantly different
kruskal.test(closeness ~ category, data = df) #p < 0.05 so categories are significantly different
kruskal.test(eigenvector ~ category, data = df) #p > 0.05 so categories are NOT significantly different

#Wilcox test to look for differences between categories non-parametrix
rare_data <- df[ which(df$category == "rare"),]
abundant_data <- df[ which(df$category == "abundant"),]
intermediate_data <- df[ which(df$category == "intermediate"),]

#Degree
# wilcox.test(rare_data$degree, abundant_data$degree, alternative = "two.sided") 
# wilcox.test(rare_data$degree, intermediate_data$degree, alternative = "two.sided") 
# wilcox.test(abundant_data$degree, intermediate_data$degree, alternative = "two.sided")

#Betweenness
wilcox.test(rare_data$betweenness, abundant_data$betweenness, alternative = "two.sided") #p < 0.05 so significantly different
wilcox.test(rare_data$betweenness, intermediate_data$betweenness, alternative = "two.sided") #p < 0.05 so significantly different
wilcox.test(abundant_data$betweenness, intermediate_data$betweenness, alternative = "two.sided") #p > 0.05 so NOT significantly different

#Closeness
wilcox.test(rare_data$closeness, abundant_data$closeness, alternative = "two.sided") #p < 0.05 so significantly different
wilcox.test(rare_data$closeness, intermediate_data$closeness, alternative = "two.sided") #p < 0.05 so significantly different
wilcox.test(abundant_data$closeness, intermediate_data$closeness, alternative = "two.sided") #p > 0.05 so NOT significantly different

#Eigenvector
# wilcox.test(rare_data$eigenvector, abundant_data$eigenvector, alternative = "two.sided") 
# wilcox.test(rare_data$eigenvector, intermediate_data$eigenvector, alternative = "two.sided") 
# wilcox.test(abundant_data$eigenvector, intermediate_data$eigenvector, alternative = "two.sided") 
```

Step Twelve: Generate boxplots of node-level properties for subcommunities
```{r}
#Degree
# Perorm pairwise comparisons
compare_means(degree ~ category,  data = df)
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("abundant", "intermediate"), c("intermediate", "rare"), c("abundant", "rare") )
ggboxplot(df, x = "category", y = "degree",
          color = "category", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y = 80)     # Add global p-value

#Between
# Perorm pairwise comparisons
compare_means(betweenness ~ category,  data = df)
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("abundant", "intermediate"), c("intermediate", "rare"), c("abundant", "rare") )
ggboxplot(df, x = "category", y = "betweenness",
          color = "category", palette = "jco")+ 
  ylim(0,60)+
  stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y = 600)# Add global p-value

#Closeness
# Perorm pairwise comparisons
compare_means(closeness ~ category,  data = df)
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("abundant", "intermediate"), c("intermediate", "rare"), c("abundant", "rare") )
ggboxplot(df, x = "category", y = "closeness",
          color = "category", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y = 0.0015)# Add global p-value

#Eigenvector
# Perform pairwise comparisons
compare_means(eigenvector ~ category,  data = df)
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("abundant", "intermediate"), c("intermediate", "rare"), c("abundant", "rare") )
ggboxplot(df, x = "category", y = "eigenvector",
          color = "category", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y=1.5)# Add global p-value
```

Step Thirteen: Identify keystone ASVs
```{r}
#Keystone species in each co-occurrence network were defined as those with a high degree of ranking in the top 20% among all nodes, and low betweenness centrality, ranking in the bottom 20% among nodes with high degree (Berry & Widder, 2014).

#top 20% degree
keystone_degree <- as.data.frame(df) %>%
        top_frac(n = 0.2, wt = degree)

#botton 20% betweenness
keystone <- as.data.frame(keystone_degree) %>%
        top_frac(n = -0.2, wt = betweenness)

#get taxa info of keystone ASVs
taxonomy <- data.frame(tax_table(arctic.filter))
taxonomy2 <- data.frame(tax_table(arctic))
taxonomy2[c('b49f507167f13204de89c7675ba3cd31'),]
keystone_taxa <- subset(taxonomy, rownames(taxonomy) %in% rownames(keystone))

NA_taxa <- taxonomy[is.na(taxonomy$Phylum),]



```

Step Fourteen: Generate random networks
```{r}
#generate 1000 random networks with the same number of nodes (asvs) and edges (connections) as our real network 
set.seed(1000)
gs <- list()
for (x in 1:1000) {
  gs[[x]] <- erdos.renyi.game(gorder(cor_g), gsize(cor_g), type = "gnm", directed = FALSE,loops = FALSE)
}
```

Step Fifteen: Calculate network-level properties for random networks
```{r}
#mean node degree (number of edges), clustering coefficient (probability that the adjacent vertices of a vertex are connected), average path length, modularity, density, network diameter 
#create a dataframe to store out network features
net_df2<-as.data.frame(matrix(NA,ncol=6,nrow=1000))
colnames(net_df2)<-c("AveDegree","ClustCoef","AvePathLen","Density","Diameter","Modularity")
for (x in 1:1000){
  net_df2[x,]<-c(mean(degree(gs[[x]])),transitivity(gs[[x]]),average.path.length(gs[[x]]),graph.density(gs[[x]]),diameter(gs[[x]]),modularity(walktrap.community(gs[[x]])))
}

```

Step Sixteen: Compare the network-level properties of the real and random networks
```{r}
#compare modularity, clustering coefficient and average path length of 1000 random networks to real network
#boxplot of modularity, clustering coefficient and avergae path length with real network values as lines on graph
#Modularity
ggboxplot(net_df2, y = "Modularity")+
  geom_hline(yintercept=net_df$Modularity[1], linetype="dashed", color = "red")

#Clustering Coefficient
ggboxplot(net_df2, y = "ClustCoef")+
  geom_hline(yintercept=net_df$ClustCoef[1], linetype="dashed", color = "red")

#Clustering Coefficient
ggboxplot(net_df2, y = "AvePathLen")+
  geom_hline(yintercept=net_df$AvePathLen[1], linetype="dashed", color = "red")

#The real network has higher values for these three topological features than the randomly generated networks, suggesting the network had “small-world” properties and modular structure. 
```

Step Seventeen: Identify if the real network exhibits scale-free characteristics
```{r}
#find the number of edges (degrees/connections) between each node (vertex/ASV) in the network
#create a dataframe with 5 columns and a row for each node (ASV)
power_df<-as.data.frame(matrix(NA,ncol=2,nrow=2*length(degree(cor_g))))

#name the colums the node-level topological features
colnames(power_df)<-c("degree","category")

#categorise the as real or random
power_df[,2][1:length(degree(cor_g))] <-"real"
power_df[,2][(length(degree(cor_g))+1):nrow(power_df)] <-"random"

# #find the mean degrees of the random networks
# deg <- as.data.frame(matrix(NA,ncol=1000,nrow=length(degree(cor_g))))
# for (i in 1:1000){
#   deg[,i] <- degree(gs[[i]])
# }

#put the topological features into the dataframe
power_df[,1][1:length(degree(cor_g))]<-degree(cor_g)
power_df[,1][(length(degree(cor_g))+1):nrow(power_df)] <- degree(gs[[1]])
#power_df[,1][(length(degree(cor_g))+1):nrow(power_df)] <- rowMeans(deg)

#plot
real <- subset(power_df, power_df$category == "real")
random <- subset(power_df, power_df$category == "random")
real_counts <- count(real$degree)
random_counts <- count(random$degree)

plot_df <- as.data.frame(matrix(NA, ncol=3, nrow=nrow(real_counts)+nrow(random_counts)))
colnames(plot_df)<-c("degree", "count", "category")
plot_df[,3][1:nrow(real_counts)] <- rep("real", nrow(real_counts))
plot_df[,3][(nrow(real_counts)+1):nrow(plot_df)] <- rep("random", nrow(random_counts))
plot_df[,2][1:nrow(real_counts)] <- real_counts$freq
plot_df[,2][(nrow(real_counts)+1):nrow(plot_df)] <- random_counts$freq
plot_df[,1][1:nrow(real_counts)] <- real_counts$x
plot_df[,1][(nrow(real_counts)+1):nrow(plot_df)] <- random_counts$x

ggplot(plot_df, aes(x=degree, y=count, color=category)) +
  geom_point()

#Taken from http://chengjun.github.io/web_data_analysis/demo2_simulate_networks/

# plot and fit the power law distribution
fit_power_law = function(graph) {
    # calculate degree
    d = degree(graph, mode = "all")
    dd = degree.distribution(graph, mode = "all", cumulative = FALSE)
    degree = 1:max(d)
    probability = dd[-1]
    # delete blank values
    nonzero.position = which(probability != 0)
    probability = probability[nonzero.position]
    degree = degree[nonzero.position]
    reg = lm(log(probability) ~ log(degree))
    cozf = coef(reg)
    power.law.fit = function(x) exp(cozf[[1]] + cozf[[2]] * log(x))
    alpha = -cozf[[2]]
    R.square = summary(reg)$r.squared
    print(paste("Alpha =", round(alpha, 3)))
    print(paste("R square =", round(R.square, 3)))
    # plot
    plot(probability ~ degree, log = "xy", xlab = "Degree (log)", ylab = "Probability (log)", 
        col = 1, main = "Degree Distribution")
    curve(power.law.fit, col = "red", add = T, n = length(d))
}


fit_power_law(cor_g)

#generate a scale-free network
g = barabasi.game(100)
fit_power_law(g) #we can see here that a scale-free network has a much higher R-Squared value 
```
