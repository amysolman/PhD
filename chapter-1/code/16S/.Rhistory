mutate(property = "Mg")
Ca.crlg <- data.frame(Ca.corlg$mantel.res) %>%
mutate(property = "Ca")
crlg <- rbind(Cond.crlg, pH.crlg, Cl.crlg, SO4.crlg, Na.crlg, K.crlg, Mg.crlg, Ca.crlg) %>%
mutate(sig = ifelse(Pr.corrected. <= 0.05, "significant", "non-significant")) %>%
filter(!(is.na(Pr.corrected.)))
#Save crlg table
write.table(crlg, "crlg.txt")
ggplot(data=crlg, aes(x=class.index, y=Mantel.cor)) +
geom_point(data=crlg[crlg$sig=="significant",], color = "black", size=2, shape=16) +
geom_point(data=crlg[crlg$sig=="non-significant",], color = "black",size=2, shape=1) +
geom_line(data=crlg, aes(color=property)) +
geom_hline(yintercept = 0, linetype=2) +
labs(x = "Phylogenetic distance class", y="Mantel correlation", color="Cryoconite property")+
theme(legend.text = element_text(size=12),
legend.title = element_text(size=12),
legend.position = "bottom",
axis.text = element_text(size=12),
axis.text.x = element_text(angle=45, hjust=1),
axis.title = element_text(size=14),
strip.text.x = element_text(size = 14))
#save pdf
pdf("total-mantel-correlogram.pdf")
print(p)
ggplot(data=crlg, aes(x=class.index, y=Mantel.cor)) +
geom_point(data=crlg[crlg$sig=="significant",], color = "black", size=2, shape=16) +
geom_point(data=crlg[crlg$sig=="non-significant",], color = "black",size=2, shape=1) +
geom_line(data=crlg, aes(color=property)) +
geom_hline(yintercept = 0, linetype=2) +
labs(x = "Phylogenetic distance class", y="Mantel correlation", color="Cryoconite property")+
theme(legend.text = element_text(size=12),
legend.title = element_text(size=12),
legend.position = "bottom",
axis.text = element_text(size=12),
axis.text.x = element_text(angle=45, hjust=1),
axis.title = element_text(size=14),
strip.text.x = element_text(size = 14))
# Rarefy to an even depth
set.seed(72)  # setting seed for reproducibility
# bulk.physeq.rare = rarefy_even_depth(bulk.physeq)
# Normalize read counts (this gives relative abundance)
new.ps.norm = transform_sample_counts(new.ps, function(x) x/sum(x))
# Function for calculating the βMNTD for each random null community
bMNTD_null_func <- function(i, OTU.table, tree){
tree$tip.label = sample(tree$tip.label)
bMNTD_s = comdistnt(OTU.table, cophenetic(tree), abundance.weighted = TRUE)
A <- attr(bMNTD_s, "Size")
B <- if (is.null(attr(bMNTD_s, "Labels"))) sequence(A) else attr(bMNTD_s, "Labels")
if (isTRUE(attr(bMNTD_s, "Diag"))) attr(bMNTD_s, "Diag") <- FALSE
if (isTRUE(attr(bMNTD_s, "Upper"))) attr(bMNTD_s, "Upper") <- FALSE
bMNTD_s.df = data.frame(Sample_1 = B[unlist(lapply(sequence(A)[-1], function(x) x:A))],
Sample_2 = rep(B[-length(B)], (length(B)-1):1),
bMNTD = as.vector(bMNTD_s),
rep=i)
return(bMNTD_s.df)
}
# The main function for calculating βNTI
Phylo_turnover <- function(physeq, reps, nproc){
# Extract OTU table
OTU.table = t(otu_table(physeq))
# Extract phylogenetic tree
tree = phy_tree(physeq)
# Get βMNTD between all communities
bMNTD_o = comdistnt(OTU.table, cophenetic(tree), abundance.weighted = TRUE)
A <- attr(bMNTD_o, "Size")
B <- if (is.null(attr(bMNTD_o, "Labels"))) sequence(A) else attr(bMNTD_o, "Labels")
if (isTRUE(attr(bMNTD_o, "Diag"))) attr(bMNTD_o, "Diag") <- FALSE
if (isTRUE(attr(bMNTD_o, "Upper"))) attr(bMNTD_o, "Upper") <- FALSE
bMNTD_o.df = data.frame(Sample_1 = B[unlist(lapply(sequence(A)[-1], function(x) x:A))],
Sample_2 = rep(B[-length(B)], (length(B)-1):1),
bMNTD = as.vector(bMNTD_o))
# Get βMNTD for randomized null communities
rep.list = seq(1, reps)
bMNTD_s.df.list = mclapply(rep.list, bMNTD_null_func, OTU.table=OTU.table, tree=tree, mc.cores=nproc)
# Combine all data together and calculate βNTI for each sample pair
bMNTD_s.df <- do.call("rbind", bMNTD_s.df.list)
bMNTD_s.means.df = bMNTD_s.df %>%
group_by(Sample_1, Sample_2) %>%
dplyr::summarize(mean_bMNTD = mean(bMNTD),
sd_bMNTD = sd(bMNTD))
bMNTD_o.df = inner_join(bMNTD_o.df, bMNTD_s.means.df, by=c("Sample_1", "Sample_2")) %>%
mutate(bNTI = (bMNTD - mean_bMNTD)/sd_bMNTD)
return(bMNTD_o.df)
}
# Get permutation beta null deviations
full.bNTI.df = Phylo_turnover(new.ps.norm, 10, 10)
#Save bNTI table
write.table(full.bNTI.df, "full_bNTI.txt")
# Function for calculating the distances in the null communities
RCbray_null_func <- function(i, freq.abd.df, alpha1, alpha2, N){
# Get simulated communities and distance
## initally select OTUs weighted by their frequency. The number of OTUs selected should equal the richness of the samples.
simcom1 = data.frame(table(sample(freq.abd.df$OTU, size=alpha1, replace=F, prob=freq.abd.df$freq)), stringsAsFactors = F)
colnames(simcom1) = c("OTU","simcom1")
simcom1$OTU = as.character(simcom1$OTU)
simcom1 = inner_join(simcom1, freq.abd.df, by="OTU")
simcom2 = data.frame(table(sample(freq.abd.df$OTU, size=alpha2, replace=F, prob=freq.abd.df$freq)), stringsAsFactors = F)
colnames(simcom2) = c("OTU","simcom2")
simcom2$OTU = as.character(simcom2$OTU)
simcom2 = inner_join(simcom2, freq.abd.df, by="OTU")
## Now recruit OTUs based on their abundance in the metacommunity
simcom1.abd = data.frame(table(sample(simcom1$OTU, size=N-alpha1, replace=T, prob=simcom1$p)), stringsAsFactors = F)
colnames(simcom1.abd) = c("OTU","simcom1.abd")
simcom1.abd$OTU = as.character(simcom1.abd$OTU)
simcom1 = full_join(simcom1, simcom1.abd, by="OTU") %>%
mutate(simcom1.abd = ifelse(is.na(simcom1.abd), 1, simcom1.abd)) %>%
select(OTU, simcom1.abd)
simcom2.abd = data.frame(table(sample(simcom2$OTU, size=N-alpha2, replace=T, prob=simcom2$p)), stringsAsFactors = F)
colnames(simcom2.abd) = c("OTU","simcom2.abd")
simcom2.abd$OTU = as.character(simcom2.abd$OTU)
simcom2 = full_join(simcom2, simcom2.abd, by="OTU") %>%
mutate(simcom2.abd = ifelse(is.na(simcom2.abd), 1, simcom2.abd)) %>%
select(OTU, simcom2.abd)
simcom = full_join(simcom1, simcom2, by="OTU")
simcom[is.na(simcom)] = 0
rownames(simcom) = simcom$OTU
simcom$OTU = NULL
null.dist = vegdist(t(simcom), method="bray")[1]
return(null.dist)
}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
graphics.off()
#library(phyloseq)
library(Hmisc)
library(igraph)
#library(agricolae)
library(fdrtool)
library(vegan) #rarecurve function
library(dplyr)
library(ggpubr)
library(car) #levene test
library(plyr)
pro <- readRDS("../../results/16S/phylo-objects/16S-phyloseq-object-rarefied-decontam.rds")
euk <- readRDS("../../results/18S/phylo-objects/18S-phyloseq-object-rarefied.rds")
#Filter ASVs by percentage presence in samples to simplify the network
filter_me <- function(my_df, perc){
#remove ASVs present in less than 10% samples
df <- 1*(my_df>0) #presence/abundance df
pres_vec <- vector()
asv_vec <- vector()
for (k in 1:ncol(df)){  #for each ASV
pres <- as.numeric(colSums(df)[k])/nrow(df) #find the percentage of samples the ASV is present in
pres_vec <- c(pres_vec, pres)
asv_vec <- c(asv_vec, colnames(df)[k])
}
df_2 <- data.frame(ASV=asv_vec, Present=pres_vec) #df of percentage of samples asvs are present in
trim_me <- subset(df_2, df_2$Present >= perc) #only keep ASVs in >= 10% of samples (that's at least two samples)
#use ASV IDs to subset count table
asv_pres_trim <- my_df[, (colnames(my_df) %in% trim_me$ASV)]
return(asv_pres_trim)
}
#Function testing area
# phylo1 = pro
# phylo2 = euk
# # pole = "Arctic"
#
# my_ASVs_and_classifications <- network_data_prep(pro, euk)
# #asv table
# my_ASVs_and_classifications$asvs
# #classifications
# my_ASVs_and_classifications$class
network_data_prep <- function(phylo1, phylo2, perc){
#subset by pole
# ps1.pole <- subset_samples(phylo1, Pole== pole)
# ps2.pole <- subset_samples(phylo2, Pole== pole)
ps1.pole <- phylo1
ps2.pole <- phylo2
#make sure both datasets have the same sample sizes
#if pro is bigger than euk, cut pro to be same size as euk
#if pro is smaller than euk, cut euk to be same size as pro
if (min(sample_sums(ps1.pole)) > min(sample_sums(ps2.pole))){
ps1.pole = rarefy_even_depth(ps1.pole, rngseed=1, sample.size=min(sample_sums(ps2.pole)), replace=F)
} else if (min(sample_sums(ps1.pole)) < min(sample_sums(ps2.pole))){
ps2.pole = rarefy_even_depth(ps2.pole, rngseed=1, sample.size=min(sample_sums(ps1.pole)), replace=F)
}
#transform our count data into relative abundances
rel_abun1 = transform_sample_counts(ps1.pole, function(x) x / sum(x) )
rel_abun2 = transform_sample_counts(ps2.pole, function(x) x / sum(x) )
#get our ASV tables
asv_tab1 <- data.frame(t(otu_table(rel_abun1)), check.names=FALSE)
asv_tab2 <- data.frame(t(otu_table(rel_abun2)), check.names=FALSE)
#make ASV table row names the sample names
data1 <- data.frame(sample_data(rel_abun1))
data2 <- data.frame(sample_data(rel_abun2))
rownames(asv_tab1) <- data1$Name
rownames(asv_tab2) <- data2$Name
#only keep samples that are the same across the datasets
asv_tab1.trim <- asv_tab1[rownames(asv_tab1) %in% rownames(asv_tab2),]
asv_tab2.trim <- asv_tab2[rownames(asv_tab2) %in% rownames(asv_tab1.trim),]
#order the rows
new_df1 <- asv_tab1.trim[ order(row.names(asv_tab1.trim)), ]
new_df2 <- asv_tab2.trim[ order(row.names(asv_tab2.trim)), ]
#seperate ASVs by abundance
rare1 <- new_df1[,colSums(new_df1)/nrow(new_df1)< 0.00001]
abundant1 <- new_df1[,colSums(new_df1)/nrow(new_df1)>0.0005]
intermediate1 <- new_df1[,colSums(new_df1)/nrow(new_df1)>=0.00001 & colSums(new_df1)/nrow(new_df1)<=0.0005]
rare2 <- new_df2[,colSums(new_df2)/nrow(new_df2)< 0.00001]
abundant2 <- new_df2[,colSums(new_df2)/nrow(new_df2)>0.0005]
intermediate2 <- new_df2[,colSums(new_df2)/nrow(new_df2)>=0.00001 & colSums(new_df2)/nrow(new_df2)<=0.0005]
#create a dataframe with ASV IDs and their abundance classification
abundance_class_df <- data.frame(ID = c(names(rare1), names(abundant1), names(intermediate1), names(rare2), names(abundant2), names(intermediate2)), Class=c(rep("Rare", ncol(rare1)), rep("Abundant", ncol(abundant1)), rep("Intermediate", ncol(intermediate1)), rep("Rare", ncol(rare2)), rep("Abundant", ncol(abundant2)), rep("Intermediate", ncol(intermediate2))), Kingdom=c(rep("Prokaryote", ncol(rare1)+ncol(abundant1)+ncol(intermediate1)), rep("Eukaryote", ncol(rare2)+ncol(abundant2)+ncol(intermediate2))))
#filter ASVs to those present in at least % of samples
pro_filter <- filter_me(new_df1, perc)
euk_filter <- filter_me(new_df2, perc)
#finally bind out dataframes together
pro_and_euk <- cbind(pro_filter, euk_filter)
#list for returning data
newList <- list("class" = abundance_class_df, "asvs" = pro_and_euk)
return(newList)
}
#Function testing area
# phylo1 = pro
# phylo2 = euk
get_my_network <- function(phylo1, phylo2, perc){
#get our asv table
output <- network_data_prep(phylo1, phylo2, perc)
asv_tab <- output$asvs
#find the spearmans correlations between asvs
cor_analysis <-rcorr(as.matrix(asv_tab),type="spearman")
#assign the matrix of correlations and the p-values to r and p, respectively
cor_r<-cor_analysis$r
cor_p<-cor_analysis$P
#matrix diagonals - apply the value 1 to the diagonal values of the p matrix (instead of NA as they were before)
diag(cor_p)<-1
#make p-values matrix into vector
cor_pp<-as.vector(cor_p)
#how many of our p values are < 0.01
length(cor_pp[cor_pp > 0 & cor_pp < 0.01]) #273830 a large number of our correlations are significant
#estimate false discovery rates
cor_qvalue<-fdrtool(cor_pp,statistic="pvalue")
#extract the vector with q-values (density based falese discovery rates)
cor_q<-cor_qvalue$qval
#what are our unique adjusted p values?
unique(cor_q) #lots of them! Essentially we don't want to see 1 NaN here (this means we have no significant adjusted p values)
#create a matrix with the q-values with the same number of rows and columns as p-values matrix
cor_q<-matrix(cor_q,nrow(cor_p),ncol(cor_p))
#make any value of q > 0.01 a 0
cor_q[cor_q>0.01]<-0
#make any value <= 0.01 & greater than 0 = 1
cor_q[cor_q<=0.01&cor_q>0]<-1
#let's look at the range of correlation coefficients
hist(cor_r)
#are there any with coefficients > | 0.6|?
length(cor_r[cor_r < -0.6]) #there are 288 negative correlation coefficients
length(cor_r[cor_r > 0.6]) #there 60567 positive correlation coefficients
#change the value of any correlation coefficients < | 0.6 | to 0
#cor_r[abs(cor_r)<0.6]<-0
cor_r[(cor_r)<0.6]<-0
#multiple the correlation coefficients by q-values so that insignificant coefficients become 0
cor<-cor_r*cor_q
#are there any with coefficients > | 0.6| after removing non-significant correlations?
length(cor[cor < -0.6]) #there are 288 negative correlation coefficients
length(cor[cor > 0.6]) #55378 positive correlations
#create igraph graph from adjacency matrix
cor_g <- graph.adjacency(cor, weighted=TRUE, mode="undirected")
#simplify the graph so it does not contain loops or multiple edges
cor_g <- simplify(cor_g)
#delete vertices from the graph
cor_g<-delete.vertices(cor_g,names(degree(cor_g)[degree(cor_g)==0]))
#export graph
write.graph(cor_g,"../../results/16S/network-analysis/16S-18S-network.gml", format="gml")
write.graph(cor_g,"../../results/18S/network-analysis/16S-18S-network.gml", format="gml")
return(cor_g)
}
#Function testing area
#my_graph <- get_my_network(pro, euk, 0.20)
df <- node_level_properties(phylo1, phylo2, perc)
node_level_properties <- function(phylo1, phylo2, perc){
output <- network_data_prep(phylo1, phylo2, perc)
cor_g <- get_my_network(phylo1, phylo2, perc)
#get the info on our classifications
class_data <- output$class
rare <- subset(class_data, (class_data$Class == "Rare"))
abundant <- subset(class_data, (class_data$Class == "Abundant"))
intermediate <- subset(class_data, (class_data$Class == "Intermediate"))
#create a dataframe with 5 columns and a row for each node (ASV)
df<-as.data.frame(matrix(NA,ncol=5,nrow=length(degree(cor_g))))
#make ASV IDs row names of df
rownames(df)<-names(degree(cor_g))
#name the colums the node-level topological features
colnames(df)<-c("degree","betweenness","closeness","eigenvector","category")
#categorise the ASVs as rare, abundant or intermediate
df[intersect(names(degree(cor_g)),rare$ID),5]<-"rare"
df[intersect(names(degree(cor_g)),abundant$ID),5]<-"abundant"
df[intersect(names(degree(cor_g)),intermediate$ID),5]<-"intermediate"
#get betweenness
btw<-betweenness(cor_g)
#get closeness centrality
cls<-closeness(cor_g)
#get eigenvector centrality
egv<-evcent(cor_g)
#put the topological features into the dataframe
df[,1]<-degree(cor_g)
df[,2]<-btw
df[,3]<-cls
df[,4]<-egv$vector
write.csv(df, "../../results/16S/network-analysis/16S-18S-node-level-properties.csv")
write.csv(df, "../../results/18S/network-analysis/16S-18S-node-level-properties.csv")
return(df)
}
df <- node_level_properties(phylo1, phylo2, perc)
phylo1 = pro
phylo2 = euk
perc = 0.05
df <- node_level_properties(phylo1, phylo2, perc)
#get taxa info of keystone ASVs
taxonomy1 <- data.frame(tax_table(phylo1))
taxonomy2 <- data.frame(tax_table(phylo2))
taxonomy <- rbind(taxonomy1, taxonomy2)
network_taxa <- subset(taxonomy, rownames(taxonomy) %in% rownames(df))
View(network_taxa)
write.csv(network_taxa, "../../results/16S/network-analysis/16S-18S-keystone-taxa.csv")
write.csv(network_taxa, "../../results/18S/network-analysis/16S-18S-keystone-taxa.csv")
rm(list=ls())
graphics.off()
#library(phyloseq)
library(Hmisc)
library(igraph)
#library(agricolae)
library(fdrtool)
library(vegan) #rarecurve function
library(dplyr)
library(ggpubr)
library(car) #levene test
library(plyr)
pro <- readRDS("../../results/16S/phylo-objects/16S-phyloseq-object-rarefied-decontam.rds")
euk <- readRDS("../../results/18S/phylo-objects/18S-phyloseq-object-rarefied.rds")
phylo1 = pro
phylo2 = euk
perc = 0.05
df <- node_level_properties(phylo1, phylo2, perc)
#Filter ASVs by percentage presence in samples to simplify the network
filter_me <- function(my_df, perc){
#remove ASVs present in less than 10% samples
df <- 1*(my_df>0) #presence/abundance df
pres_vec <- vector()
asv_vec <- vector()
for (k in 1:ncol(df)){  #for each ASV
pres <- as.numeric(colSums(df)[k])/nrow(df) #find the percentage of samples the ASV is present in
pres_vec <- c(pres_vec, pres)
asv_vec <- c(asv_vec, colnames(df)[k])
}
df_2 <- data.frame(ASV=asv_vec, Present=pres_vec) #df of percentage of samples asvs are present in
trim_me <- subset(df_2, df_2$Present >= perc) #only keep ASVs in >= 10% of samples (that's at least two samples)
#use ASV IDs to subset count table
asv_pres_trim <- my_df[, (colnames(my_df) %in% trim_me$ASV)]
return(asv_pres_trim)
}
network_data_prep <- function(phylo1, phylo2, perc){
#subset by pole
# ps1.pole <- subset_samples(phylo1, Pole== pole)
# ps2.pole <- subset_samples(phylo2, Pole== pole)
ps1.pole <- phylo1
ps2.pole <- phylo2
#make sure both datasets have the same sample sizes
#if pro is bigger than euk, cut pro to be same size as euk
#if pro is smaller than euk, cut euk to be same size as pro
if (min(sample_sums(ps1.pole)) > min(sample_sums(ps2.pole))){
ps1.pole = rarefy_even_depth(ps1.pole, rngseed=1, sample.size=min(sample_sums(ps2.pole)), replace=F)
} else if (min(sample_sums(ps1.pole)) < min(sample_sums(ps2.pole))){
ps2.pole = rarefy_even_depth(ps2.pole, rngseed=1, sample.size=min(sample_sums(ps1.pole)), replace=F)
}
#transform our count data into relative abundances
rel_abun1 = transform_sample_counts(ps1.pole, function(x) x / sum(x) )
rel_abun2 = transform_sample_counts(ps2.pole, function(x) x / sum(x) )
#get our ASV tables
asv_tab1 <- data.frame(t(otu_table(rel_abun1)), check.names=FALSE)
asv_tab2 <- data.frame(t(otu_table(rel_abun2)), check.names=FALSE)
#make ASV table row names the sample names
data1 <- data.frame(sample_data(rel_abun1))
data2 <- data.frame(sample_data(rel_abun2))
rownames(asv_tab1) <- data1$Name
rownames(asv_tab2) <- data2$Name
#only keep samples that are the same across the datasets
asv_tab1.trim <- asv_tab1[rownames(asv_tab1) %in% rownames(asv_tab2),]
asv_tab2.trim <- asv_tab2[rownames(asv_tab2) %in% rownames(asv_tab1.trim),]
#order the rows
new_df1 <- asv_tab1.trim[ order(row.names(asv_tab1.trim)), ]
new_df2 <- asv_tab2.trim[ order(row.names(asv_tab2.trim)), ]
#seperate ASVs by abundance
rare1 <- new_df1[,colSums(new_df1)/nrow(new_df1)< 0.00001]
abundant1 <- new_df1[,colSums(new_df1)/nrow(new_df1)>0.0005]
intermediate1 <- new_df1[,colSums(new_df1)/nrow(new_df1)>=0.00001 & colSums(new_df1)/nrow(new_df1)<=0.0005]
rare2 <- new_df2[,colSums(new_df2)/nrow(new_df2)< 0.00001]
abundant2 <- new_df2[,colSums(new_df2)/nrow(new_df2)>0.0005]
intermediate2 <- new_df2[,colSums(new_df2)/nrow(new_df2)>=0.00001 & colSums(new_df2)/nrow(new_df2)<=0.0005]
#create a dataframe with ASV IDs and their abundance classification
abundance_class_df <- data.frame(ID = c(names(rare1), names(abundant1), names(intermediate1), names(rare2), names(abundant2), names(intermediate2)), Class=c(rep("Rare", ncol(rare1)), rep("Abundant", ncol(abundant1)), rep("Intermediate", ncol(intermediate1)), rep("Rare", ncol(rare2)), rep("Abundant", ncol(abundant2)), rep("Intermediate", ncol(intermediate2))), Kingdom=c(rep("Prokaryote", ncol(rare1)+ncol(abundant1)+ncol(intermediate1)), rep("Eukaryote", ncol(rare2)+ncol(abundant2)+ncol(intermediate2))))
#filter ASVs to those present in at least % of samples
pro_filter <- filter_me(new_df1, perc)
euk_filter <- filter_me(new_df2, perc)
#finally bind out dataframes together
pro_and_euk <- cbind(pro_filter, euk_filter)
#list for returning data
newList <- list("class" = abundance_class_df, "asvs" = pro_and_euk)
return(newList)
}
get_my_network <- function(phylo1, phylo2, perc){
#get our asv table
output <- network_data_prep(phylo1, phylo2, perc)
asv_tab <- output$asvs
#find the spearmans correlations between asvs
cor_analysis <-rcorr(as.matrix(asv_tab),type="spearman")
#assign the matrix of correlations and the p-values to r and p, respectively
cor_r<-cor_analysis$r
cor_p<-cor_analysis$P
#matrix diagonals - apply the value 1 to the diagonal values of the p matrix (instead of NA as they were before)
diag(cor_p)<-1
#make p-values matrix into vector
cor_pp<-as.vector(cor_p)
#how many of our p values are < 0.01
length(cor_pp[cor_pp > 0 & cor_pp < 0.01]) #273830 a large number of our correlations are significant
#estimate false discovery rates
cor_qvalue<-fdrtool(cor_pp,statistic="pvalue")
#extract the vector with q-values (density based falese discovery rates)
cor_q<-cor_qvalue$qval
#what are our unique adjusted p values?
unique(cor_q) #lots of them! Essentially we don't want to see 1 NaN here (this means we have no significant adjusted p values)
#create a matrix with the q-values with the same number of rows and columns as p-values matrix
cor_q<-matrix(cor_q,nrow(cor_p),ncol(cor_p))
#make any value of q > 0.01 a 0
cor_q[cor_q>0.01]<-0
#make any value <= 0.01 & greater than 0 = 1
cor_q[cor_q<=0.01&cor_q>0]<-1
#let's look at the range of correlation coefficients
hist(cor_r)
#are there any with coefficients > | 0.6|?
length(cor_r[cor_r < -0.6]) #there are 288 negative correlation coefficients
length(cor_r[cor_r > 0.6]) #there 60567 positive correlation coefficients
#change the value of any correlation coefficients < | 0.6 | to 0
#cor_r[abs(cor_r)<0.6]<-0
cor_r[(cor_r)<0.6]<-0
#multiple the correlation coefficients by q-values so that insignificant coefficients become 0
cor<-cor_r*cor_q
#are there any with coefficients > | 0.6| after removing non-significant correlations?
length(cor[cor < -0.6]) #there are 288 negative correlation coefficients
length(cor[cor > 0.6]) #55378 positive correlations
#create igraph graph from adjacency matrix
cor_g <- graph.adjacency(cor, weighted=TRUE, mode="undirected")
#simplify the graph so it does not contain loops or multiple edges
cor_g <- simplify(cor_g)
#delete vertices from the graph
cor_g<-delete.vertices(cor_g,names(degree(cor_g)[degree(cor_g)==0]))
#export graph
write.graph(cor_g,"../../results/16S/network-analysis/16S-18S-network.gml", format="gml")
write.graph(cor_g,"../../results/18S/network-analysis/16S-18S-network.gml", format="gml")
return(cor_g)
}
# #Function testing area
# phylo1 = pro
# phylo2 = euk
# perc=0.05
#
# node_df <-node_level_properties(pro, euk, 0.05)
node_level_properties <- function(phylo1, phylo2, perc){
output <- network_data_prep(phylo1, phylo2, perc)
cor_g <- get_my_network(phylo1, phylo2, perc)
#get the info on our classifications
class_data <- output$class
rare <- subset(class_data, (class_data$Class == "Rare"))
abundant <- subset(class_data, (class_data$Class == "Abundant"))
intermediate <- subset(class_data, (class_data$Class == "Intermediate"))
#create a dataframe with 5 columns and a row for each node (ASV)
df<-as.data.frame(matrix(NA,ncol=5,nrow=length(degree(cor_g))))
#make ASV IDs row names of df
rownames(df)<-names(degree(cor_g))
#name the colums the node-level topological features
colnames(df)<-c("degree","betweenness","closeness","eigenvector","category")
#categorise the ASVs as rare, abundant or intermediate
df[intersect(names(degree(cor_g)),rare$ID),5]<-"rare"
df[intersect(names(degree(cor_g)),abundant$ID),5]<-"abundant"
df[intersect(names(degree(cor_g)),intermediate$ID),5]<-"intermediate"
#get betweenness
btw<-betweenness(cor_g)
#get closeness centrality
cls<-closeness(cor_g)
#get eigenvector centrality
egv<-evcent(cor_g)
#put the topological features into the dataframe
df[,1]<-degree(cor_g)
df[,2]<-btw
df[,3]<-cls
df[,4]<-egv$vector
write.csv(df, "../../results/16S/network-analysis/16S-18S-node-level-properties.csv")
write.csv(df, "../../results/18S/network-analysis/16S-18S-node-level-properties.csv")
return(df)
}
phylo1 = pro
phylo2 = euk
perc = 0.05
f <- node_level_properties(phylo1, phylo2, perc)
df <- node_level_properties(phylo1, phylo2, perc)
#get taxa info of keystone ASVs
taxonomy1 <- data.frame(tax_table(phylo1))
taxonomy2 <- data.frame(tax_table(phylo2))
taxonomy <- rbind(taxonomy1, taxonomy2)
network_taxa <- subset(taxonomy, rownames(taxonomy) %in% rownames(df))
write.csv(network_taxa, "../../results/16S/network-analysis/16S-18S-taxa.csv")
write.csv(network_taxa, "../../results/18S/network-analysis/16S-18S-taxa.csv")
keystone_degree <- as.data.frame(df) %>%
top_frac(n = 0.1, wt = degree)
#botton 10% betweenness
keystone <- as.data.frame(keystone_degree) %>%
top_frac(n = -0.1, wt = betweenness)
#get taxa info of keystone ASVs
taxonomy1 <- data.frame(tax_table(phylo1))
taxonomy2 <- data.frame(tax_table(phylo2))
taxonomy <- rbind(taxonomy1, taxonomy2)
keystone_taxa <- subset(taxonomy, rownames(taxonomy) %in% rownames(keystone))
write.csv(keystone_taxa, "../../results/16S/network-analysis/16S-18S-keystone-taxa.csv")
write.csv(keystone_taxa, "../../results/18S/network-analysis/16S-18S-keystone-taxa.csv")
