---
title: "Neutral Model"
author: "Amy Solman"
date: "12/10/2021"
output: html_document
---
This script will fit Sloan's Neutral Model to my data.

What is Sloan's Neutral Model?
Sloan's Neutral Model is a mathematical model that explains observed relation abundance (percentage of community a taxa represents) and the frequency with which that taxa is observed (the number of samples it is found in) can be explained by a neutral community model (NCM).

The NCM is a stochastic, birth-death immigration process. IT reproduces the observed species abundance distribution.

If a community are RANDOMLY ASSEMBLED then the frequency with which taxa are observed (the percentage of samples they are found in) should increase as a function of the average abundance of taxa across all samples.

We use it to assess whether the observed composition of communities is consistent with neutral community assembly.

How do you fit a neutral model to your data?
1 Clear workspace and install packages
2 Read in data
1 Get ASV count table
2 Get the mean relative abundance of each ASV across all samples
3 Calculate frequency data for each ASV
4 Put relative abundance and frequency data into dataframe for plotting
5 Fit the neutral model
6 Get ASV frequencies as predicted by stochastic processes
7 Calculate RSquared value for model fitting and model fit stats
8 Get data for plotting
9 Plot the model fit

Code developed from this paper:
Contribution of neutral processes to the assembly of gut microbial communities in the zebrafish over host development
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Install Packages
```{r}
rm(list=ls())
##Fits the neutral model from Sloan et al. 2006 to an OTU table and returns several fitting statistics. Alternatively, will return predicted occurrence frequencies for each OTU based on their abundance in the metacommunity
#Install the following packages if they haven't been availabled in your computer yet 
library(Hmisc) #binconf function
library(minpack.lm) #for non-linear model fitting
# library(stats4) #mle function
library(dplyr) #for %>% function
library(tibble) #function rownames_to_column
#install.packages("GUniFrac") #for Rarefy funtion
library(GUniFrac)
library(phyloseq)
```

Load data 
```{r}
#TOTAL DATASET
ps <- readRDS("../../results/16S/phylo-objects/16S-phyloseq-object-rarefied-decontam.rds")

# #Total Abundant
# ps.abun <- readRDS("../../results/16S/phylo-objects/16S-total-abundant.rds")
# #Total Intermediate
# ps.int <- readRDS("../../results/16S/phylo-objects/16S-total-intermediate.rds")
# #Total Rare
# ps.rare <- readRDS("../../results/16S/phylo-objects/16S-total-rare.rds")
```
Function for getting count data and aggregating by taxonomic classification
```{r}
# agg_my_counts <- function(phylo, level){
#   
#   spe <- data.frame(t(otu_table(phylo)))
# 
#   #get out taxonomy table
#   tax_tab <- data.frame(tax_table(phylo))
#   #replace NAs with Unknown
#   tax_tab[is.na(tax_tab)] <- "Unknown"
#   colnames(spe) <- tax_tab[, level]
# 
#   #aggregate count data
#   x <- t(spe)
#   new_df=aggregate(x, by=list(rownames(x)),sum)
#   new_counts <- t(new_df)
#   colnames(new_counts) <- new_counts[1,]
#   new_counts <- new_counts[-1,]
#   df <- data.frame(new_counts)
#   #make dataframe numeric
#   df[] <- lapply(df, as.numeric)
#   df <- df[,! names(df) %in% c("Unknown")]
#   
#   
#   return(df)
# }

#Function testing area
# phylo = ps
# level = "Phylum"
# df1 <- agg_my_counts(ps, "Phylum")
```

Function for identifying abundance of ASVs
```{r}
# #function testingarea
# # phylo = ps
# 
# find_my_abundance <- function(phylo){
#   
# #transform to relative abundance data
# rel.abun  = transform_sample_counts(phylo, function(x) x / sum(x) )
# 
# #sample sums should all now = 1
# sample_sums(rel.abun)
# 
# #subset data by pole
# ps.rel.arc <- subset_samples(rel.abun, Pole == "Arctic")
# ps.rel.ant <- subset_samples(rel.abun, Pole == "Antarctic")
# 
# #remove ASVs with 0 relative abundance
# ps.rel.arc.filter <- filter_taxa(ps.rel.arc, function(x) sum(x) > 0, TRUE)
# ps.rel.ant.filter <- filter_taxa(ps.rel.ant, function(x) sum(x) > 0, TRUE)
# 
# #RARE ARCTIC: only keep taxa with mean local relative abundance less than 0.001%
# ps.rel.arc.filter.rare <- filter_taxa(ps.rel.arc.filter, function(x) mean(x) < 0.00001, TRUE)
# #RARE ANTARCTIC
# ps.rel.ant.filter.rare <- filter_taxa(ps.rel.ant.filter, function(x) mean(x) < 0.00001, TRUE)
# 
# #ABUNDANT ARCTIC: only keep taxa with mean local relative abundance more than 0.05%
# ps.rel.arc.filter.abun <- filter_taxa(ps.rel.arc.filter, function(x) mean(x) > 0.0005, TRUE)
# #ABUNDANT ANTARCTIC
# ps.rel.ant.filter.abun <- filter_taxa(ps.rel.ant.filter, function(x) mean(x) > 0.0005, TRUE)
# 
# #INTERMEDIATE ARCTIC: only keep taxa with mean local relative abundance more than 0.001% and less than 0.05%
# ps.rel.arc.filter.int <- filter_taxa(ps.rel.arc.filter, function(x) mean(x) > 0.00001 & mean(x) < 0.0005, TRUE)
# #INTERMEDIATE ANTARCTIC
# ps.rel.ant.filter.int <- filter_taxa(ps.rel.ant.filter, function(x) mean(x) > 0.00001 & mean(x) < 0.0005, TRUE)
# 
# #create dataframe with pole, ASV ID, taxonomic classification and abundance
# 
# #arctic abundance taxa
# arctic_rare_tax <- data.frame(tax_table(ps.rel.arc.filter.rare))
# arctic_rare_tax$Abundance <- "Rare"
# arctic_rare_tax$Pole <- "Arctic"
# arctic_rare_tax$ID <- rownames(arctic_rare_tax)
# 
# arctic_int_tax <- data.frame(tax_table(ps.rel.arc.filter.int))
# arctic_int_tax$Abundance <- "Intermediate"
# arctic_int_tax$Pole <- "Arctic"
# arctic_int_tax$ID <- rownames(arctic_int_tax)
# 
# arctic_abun_tax <- data.frame(tax_table(ps.rel.arc.filter.abun))
# arctic_abun_tax$Abundance <- "Abundant"
# arctic_abun_tax$Pole <- "Arctic"
# arctic_abun_tax$ID <- rownames(arctic_abun_tax)
# 
# arctic_data <- rbind(arctic_rare_tax, arctic_int_tax, arctic_abun_tax)
# 
# 
# #antarctic abundance taxa
# antarctic_rare_tax <- data.frame(tax_table(ps.rel.ant.filter.rare))
# antarctic_rare_tax$Abundance <- "Rare"
# antarctic_rare_tax$Pole <- "Antarctic"
# antarctic_rare_tax$ID <- rownames(antarctic_rare_tax)
# 
# antarctic_int_tax <- data.frame(tax_table(ps.rel.ant.filter.int))
# antarctic_int_tax$Abundance <- "Intermediate"
# antarctic_int_tax$Pole <- "Antarctic"
# antarctic_int_tax$ID <- rownames(antarctic_int_tax)
# 
# antarctic_abun_tax <- data.frame(tax_table(ps.rel.ant.filter.abun))
# antarctic_abun_tax$Abundance <- "Abundant"
# antarctic_abun_tax$Pole <- "Antarctic"
# antarctic_abun_tax$ID <- rownames(antarctic_abun_tax)
# 
# antarctic_data <- rbind(antarctic_rare_tax, antarctic_int_tax, antarctic_abun_tax)
# 
# #bind the dataframes
# total_data <- rbind(arctic_data, antarctic_data)
# 
# return(total_data)
# 
# }
```

An alternative way to classify abundance

Interaction and assembly processes of abundant and rare microbial communities during a diatom bloom process
OTUs with relative abundances above 1% in one sample were classiﬁed as abundant’ according to previouss tudy(Liuet al., 2015). OTUs with relative abundance less than 0.1% across all samples were deﬁned as ‘rare’, and those with intermediate abundance were d eﬁned as ‘common’.
```{r}
# find_my_abundance_alt <- function(phylo){
#   
# #transform to relative abundance data
# rel.abun  = transform_sample_counts(phylo, function(x) x / sum(x) )
# 
# #sample sums should all now = 1
# sample_sums(rel.abun)
# 
# #subset data by pole
# ps.rel.arc <- subset_samples(rel.abun, Pole == "Arctic")
# ps.rel.ant <- subset_samples(rel.abun, Pole == "Antarctic")
# 
# #remove ASVs with 0 relative abundance
# ps.rel.arc.filter <- filter_taxa(ps.rel.arc, function(x) sum(x) > 0, TRUE)
# ps.rel.ant.filter <- filter_taxa(ps.rel.ant, function(x) sum(x) > 0, TRUE)
# 
# #Arctic
# #get otu table
# arctic_otus <- data.frame(otu_table(ps.rel.arc.filter))
# #identify abundances
# arc_abun<- arctic_otus %>% filter_at(vars(1:ncol(arctic_otus)), any_vars(. > 0.01)) #abundant = ASV with more than 1% abundance in any sample
# arc_rare<- arctic_otus %>% filter_at(vars(1:ncol(arctic_otus)), all_vars(. < 0.001)) #rare = ASV with less than 0.1% abundance in all samples
# arc_int <- arctic_otus[! (rownames(arctic_otus) %in% rownames(arc_abun)),]
# arc_int <- arc_int[!(rownames(arc_int) %in% rownames(arc_rare)),]
# 
# #Anrarctic
# #get otu table
# antarctic_otus <- data.frame(otu_table(ps.rel.ant.filter))
# #identify abundances
# ant_abun<- antarctic_otus %>% filter_at(vars(1:ncol(antarctic_otus)), any_vars(. > 0.01))
# ant_rare<- antarctic_otus %>% filter_at(vars(1:ncol(antarctic_otus)), all_vars(. < 0.001))
# ant_int <- antarctic_otus[! (rownames(antarctic_otus) %in% rownames(ant_abun)),]
# ant_int <- ant_int[!(rownames(ant_int) %in% rownames(ant_rare)),]
# 
# #create dataframe with pole, ASV ID, taxonomic classification and abundance
# 
# #arctic abundance taxa
# #arctic_rare_tax <- data.frame(tax_table(ps.rel.arc.filter.rare))
# arc_rare_data <- data.frame(ID=rownames(arc_rare))
# arc_rare_data$Abundance <- "Rare"
# arc_rare_data$Pole <- "Arctic"
# 
# #arctic_int_tax <- data.frame(tax_table(ps.rel.arc.filter.int))
# arc_int_data <- data.frame(ID=rownames(arc_int))
# arc_int_data$Abundance <- "Intermediate"
# arc_int_data$Pole <- "Arctic"
# 
# #arctic_abun_tax <- data.frame(tax_table(ps.rel.arc.filter.abun))
# arc_abun_data <- data.frame(ID=rownames(arc_abun))
# arc_abun_data$Abundance <- "Abundant"
# arc_abun_data$Pole <- "Arctic"
# 
# arctic_data <- rbind(arc_rare_data, arc_int_data, arc_abun_data)
# 
# 
# #antarctic abundance taxa
# #arctic_rare_tax <- data.frame(tax_table(ps.rel.arc.filter.rare))
# ant_rare_data <- data.frame(ID=rownames(ant_rare))
# ant_rare_data$Abundance <- "Rare"
# ant_rare_data$Pole <- "Antarctic"
# 
# #arctic_int_tax <- data.frame(tax_table(ps.rel.arc.filter.int))
# ant_int_data <- data.frame(ID=rownames(ant_int))
# ant_int_data$Abundance <- "Intermediate"
# ant_int_data$Pole <- "Antarctic"
# 
# #arctic_abun_tax <- data.frame(tax_table(ps.rel.arc.filter.abun))
# ant_abun_data <- data.frame(ID=rownames(ant_abun))
# ant_abun_data$Abundance <- "Abundant"
# ant_abun_data$Pole <- "Antarctic"
# 
# antarctic_data <- rbind(ant_rare_data, ant_int_data, ant_abun_data)
# 
# #bind the dataframes
# total_data <- rbind(arctic_data, antarctic_data)
# 
# return(total_data)
# 
# }
```

Another way of classifying abundance
```{r}
find_my_abundance_alt_2 <- function(phylo){
  
#transform to relative abundance data
rel.abun  = transform_sample_counts(phylo, function(x) x / sum(x) )

#sample sums should all now = 1
sample_sums(rel.abun)

#subset data by pole
# ps.rel.arc <- subset_samples(rel.abun, Pole == "Arctic")
# ps.rel.ant <- subset_samples(rel.abun, Pole == "Antarctic")

#remove ASVs with 0 relative abundance
ps.rel.filter <- filter_taxa(rel.abun, function(x) sum(x) > 0, TRUE)
# ps.rel.ant.filter <- filter_taxa(ps.rel.ant, function(x) sum(x) > 0, TRUE)


#get otu table
otus <- data.frame(otu_table(ps.rel.filter))
#identify abundances
abun<- otus %>% filter_at(vars(1:ncol(otus)), any_vars(. > 0.01)) #abundant = ASV with more than 1% abundance in any sample
rare<- otus %>% filter_at(vars(1:ncol(otus)), all_vars(. < 0.001)) #rare = ASV with less than 0.1% abundance in all samples
int <- otus[! (rownames(otus) %in% rownames(abun)),]
int <- int[!(rownames(int) %in% rownames(rare)),]

# #Anrarctic
# #get otu table
# antarctic_otus <- data.frame(otu_table(ps.rel.ant.filter))
# #identify abundances
# ant_abun<- antarctic_otus %>% filter_at(vars(1:ncol(antarctic_otus)), any_vars(. > 0.01))
# ant_rare<- antarctic_otus %>% filter_at(vars(1:ncol(antarctic_otus)), all_vars(. < 0.001))
# ant_int <- antarctic_otus[! (rownames(antarctic_otus) %in% rownames(ant_abun)),]
# ant_int <- ant_int[!(rownames(ant_int) %in% rownames(ant_rare)),]

#create dataframe with pole, ASV ID, taxonomic classification and abundance

#arctic abundance taxa
#arctic_rare_tax <- data.frame(tax_table(ps.rel.arc.filter.rare))
rare_data <- data.frame(ID=rownames(rare))
rare_data$Abundance <- "Rare"
# arc_rare_data$Pole <- "Arctic"

#arctic_int_tax <- data.frame(tax_table(ps.rel.arc.filter.int))
int_data <- data.frame(ID=rownames(int))
int_data$Abundance <- "Intermediate"
# arc_int_data$Pole <- "Arctic"

#arctic_abun_tax <- data.frame(tax_table(ps.rel.arc.filter.abun))
abun_data <- data.frame(ID=rownames(abun))
abun_data$Abundance <- "Abundant"
# arc_abun_data$Pole <- "Arctic"

total_data <- rbind(rare_data, int_data, abun_data)


# #antarctic abundance taxa
# #arctic_rare_tax <- data.frame(tax_table(ps.rel.arc.filter.rare))
# ant_rare_data <- data.frame(ID=rownames(ant_rare))
# ant_rare_data$Abundance <- "Rare"
# ant_rare_data$Pole <- "Antarctic"
# 
# #arctic_int_tax <- data.frame(tax_table(ps.rel.arc.filter.int))
# ant_int_data <- data.frame(ID=rownames(ant_int))
# ant_int_data$Abundance <- "Intermediate"
# ant_int_data$Pole <- "Antarctic"
# 
# #arctic_abun_tax <- data.frame(tax_table(ps.rel.arc.filter.abun))
# ant_abun_data <- data.frame(ID=rownames(ant_abun))
# ant_abun_data$Abundance <- "Abundant"
# ant_abun_data$Pole <- "Antarctic"
# 
# antarctic_data <- rbind(ant_rare_data, ant_int_data, ant_abun_data)
# 
# #bind the dataframes
# total_data <- rbind(arctic_data, antarctic_data)

return(total_data)

}
```


Function for plotting neutral model
```{r}
#Testing area for function
# phylo = ps
# pole = "Arctic"
# 
# neutral_model_plot <- function(phylo, pole){
#   
#   #firstly we can get the abundance classifications of all our ASVs
#   abun_class <- find_my_abundance_alt(phylo)
#   
#   #subset phyloseq object by pole
#   new_phylo <- subset_samples(phylo, Pole == pole)
#   
#   #remove ASVs with zero reads in new phyloseq object
#   new_phylo.prune <- prune_taxa(taxa_sums(new_phylo) > 0, new_phylo) 
#   
#   #subset out abundance data by pole
#   abun_class_sub <- abun_class[ which(abun_class$Pole== pole),]
# 
# # Extract the ASV table from the phyloseq object
# ASV.table = data.frame(t(otu_table(new_phylo.prune)), check.names = FALSE)
# 
# #get the mean number of reads per sample (mean sum of each row)
# N <- mean(apply(ASV.table, 1, sum)) #this is the number of reads in each sample 
# 
# #get the mean number of reads for each ASV across all samples (mean of each column)
# p.m <- apply(ASV.table, 2, mean) #what is the mean number of times an ASV appears in each sample
# 
# #Remove any zeros
# p.m <- p.m[p.m != 0] #remove any ASVs with zero counts
# 
# #divide the number of mean reads by the total number of reads per sample (mean relative abundance of each ASV globally)
# p <- p.m/N #mean relative abundance of each ASV
# 
# #make ASV.table into presence/absence table
# ASV.table.bi <- 1*(ASV.table>0)
# 
# #find the mean of frequence of each column (ASV) e.g. the mean number of samples each ASV is found in
# freq.table <- apply(ASV.table.bi, 2, mean) 
# 
# #only keep ASVs with a frequency other than 0
# freq.table <- freq.table[freq.table != 0] #only keep data that isn't zero
# 
# #put the average relative abundance of each taxa into a dataframe
# p.df = data.frame(p) %>%
#   rownames_to_column(var="ASV") 
# 
# #Make into dataframe with ASV name and frequence of occurence (percentage of samples the ASV is present in )
# freq.df = data.frame(ASV=names(freq.table), freq=freq.table) 
# 
# #Combine dataframes and arrange by relative abundance 
# C <- inner_join(p.df,freq.df, by="ASV") %>%
#   arrange(p)
# 
# # Remove rows with any zero (absent in either source pool or local communities). You already did this, but just to make sure we will do it again.
# C.no0 <- C %>%
#   filter(freq != 0, p != 0)
# 
# #Calculate the limit of detection
# d <- 1/N
# 
# #get vectors of our mean relative abundances and frequencies
# p.list <- C.no0$p #this creates a vector of the mean relative abundances of each ASV
# freq.list <- C.no0$freq #this creates a list of the mean frequency of each ASV
# 
# ##Fit model parameter m (immigration rate) using Non-linear least squares (NLS)
# m.fit <- nlsLM(freq.list ~ pbeta(d, N*m*p.list, N*m*(1-p.list), lower.tail=FALSE), start=list(m=0.001)) #using the mean relative abundances of each ASV, the mean frequency of each ASV, the size of the metacommunity and the detection limit, estimate the dispersal rate of the study area using non-linear least squares fitting.
# 
# freq.pred <- pbeta(d, N*coef(m.fit)*p.list, N*coef(m.fit)*(1-p.list), lower.tail=FALSE) #get the predicted ASV frequencies under stochastic processes
# 
# #Get R2 value
# #how different are the predicted values from our real values
# Rsqr <- 1 - (sum((freq.list - freq.pred)^2))/(sum((freq.list - mean(freq.list))^2)) #get the R2 result of observed frequencies against predicted frequencies
# 
# #get the confidence intervals for m
# m.ci <- confint(m.fit, 'm', level=0.95)
# 
# #get a summary of the model fitting
# m.sum <- summary(m.fit) 
# 
# #get the estimated coefficient of our model fitting
# m.coef = coef(m.fit) 
# 
# # Get table of model fit stats
# fitstats <- data.frame(m=m.coef, m.low.ci=m.ci[1], m.up.ci=m.ci[2],
#                        Rsqr=Rsqr, p.value=m.sum$parameters[4], N=N,
#                        Samples=nrow(ASV.table), Richness=length(p.list),
#                        Detect=d)
# 
# # Get confidence interval for predictions
# freq.pred.ci <- binconf(freq.pred*nrow(ASV.table), nrow(ASV.table), alpha=0.05, method="wilson", return.df=TRUE)
# 
# # Get table of predictions
# pred.df <- data.frame(metacomm_RA=p.list, frequency=freq.pred,
#                       frequency_lowerCI=freq.pred.ci[,2],
#                       frequency_upperCI=freq.pred.ci[,3]) %>%
#   unique()
# 
# # Get table of observed occupancy and abundance
# obs.df = C.no0 %>%
#   rename(metacomm_RA = p, frequency=freq)
# 
# #add abundance classifications to obs.df
# obs.df$abun_class <- abun_class_sub$Abundance
# abun_class_sub$ASV <- abun_class_sub$ID
# new.df <- merge(obs.df, abun_class_sub, by=c("ASV"))
# 
# p <- ggplot(data=new.df) +
#     geom_point(data=new.df, aes(x=log10(metacomm_RA), y=frequency, color=Abundance),
#                alpha=.2, size=2) +
#     geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency), color="black") +
#     geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency_lowerCI), linetype=2, color="black") +
#     geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency_upperCI), linetype=2, color="black") +
#     geom_text(data=fitstats, aes(label = paste("R^2 == ", round(Rsqr, 3))),
#               x=-4.9, y=0.75, size=4, parse=TRUE) +
#     geom_text(data=fitstats, aes(label = paste("italic(m) ==", round(m, 4))),
#               x=-4.9, y=0.68, size=4, parse=TRUE) +
#     labs(x="Log10 abundance in\nmetacommunity", y="Frequency detected") +
#     theme_bw() +
#     theme(axis.line = element_line(color="black"),
#           #legend.position = "none",
#           axis.title = element_text(size=14),
#           axis.text = element_text(size=12))+
#   ggtitle(paste0(pole, " Neutral Model Plot"))
# 
# #save plot
#     pdf(paste0("../../results/16S/graphs/neutral-model/", pole, "-neutral-model.pdf"))
#     print(p)
#     dev.off()
# }

```

Function for plotting neutral model both poles
```{r}
#Testing area for function
#phylo = ps

neutral_model_plot_2 <- function(phylo){
  
  #firstly we can get the abundance classifications of all our ASVs
  abun_class <- find_my_abundance_alt_2(phylo)
  
  # #subset phyloseq object by pole
  # new_phylo <- subset_samples(phylo, Pole == pole)
  
  #remove ASVs with zero reads in new phyloseq object
  phylo.prune <- prune_taxa(taxa_sums(phylo) > 0, phylo) 
  
  #subset out abundance data by pole
  #abun_class_sub <- abun_class[ which(abun_class$Pole== pole),]

# Extract the ASV table from the phyloseq object
ASV.table = data.frame(t(otu_table(phylo.prune)), check.names = FALSE)

#get the mean number of reads per sample (mean sum of each row)
N <- mean(apply(ASV.table, 1, sum)) #this is the number of reads in each sample 

#get the mean number of reads for each ASV across all samples (mean of each column)
p.m <- apply(ASV.table, 2, mean) #what is the mean number of times an ASV appears in each sample

#Remove any zeros
p.m <- p.m[p.m != 0] #remove any ASVs with zero counts

#divide the number of mean reads by the total number of reads per sample (mean relative abundance of each ASV globally)
p <- p.m/N #mean relative abundance of each ASV

#make ASV.table into presence/absence table
ASV.table.bi <- 1*(ASV.table>0)

#find the mean of frequence of each column (ASV) e.g. the mean number of samples each ASV is found in
freq.table <- apply(ASV.table.bi, 2, mean) 

#only keep ASVs with a frequency other than 0
freq.table <- freq.table[freq.table != 0] #only keep data that isn't zero

#put the average relative abundance of each taxa into a dataframe
p.df = data.frame(p) %>%
  rownames_to_column(var="ASV") 

#Make into dataframe with ASV name and frequence of occurence (percentage of samples the ASV is present in )
freq.df = data.frame(ASV=names(freq.table), freq=freq.table) 

#Combine dataframes and arrange by relative abundance 
C <- inner_join(p.df,freq.df, by="ASV") %>%
  arrange(p)

# Remove rows with any zero (absent in either source pool or local communities). You already did this, but just to make sure we will do it again.
C.no0 <- C %>%
  filter(freq != 0, p != 0)

#Calculate the limit of detection
d <- 1/N

#get vectors of our mean relative abundances and frequencies
p.list <- C.no0$p #this creates a vector of the mean relative abundances of each ASV
freq.list <- C.no0$freq #this creates a list of the mean frequency of each ASV

##Fit model parameter m (immigration rate) using Non-linear least squares (NLS)
m.fit <- nlsLM(freq.list ~ pbeta(d, N*m*p.list, N*m*(1-p.list), lower.tail=FALSE), start=list(m=0.001)) #using the mean relative abundances of each ASV, the mean frequency of each ASV, the size of the metacommunity and the detection limit, estimate the dispersal rate of the study area using non-linear least squares fitting.

freq.pred <- pbeta(d, N*coef(m.fit)*p.list, N*coef(m.fit)*(1-p.list), lower.tail=FALSE) #get the predicted ASV frequencies under stochastic processes

#Get R2 value
#how different are the predicted values from our real values
Rsqr <- 1 - (sum((freq.list - freq.pred)^2))/(sum((freq.list - mean(freq.list))^2)) #get the R2 result of observed frequencies against predicted frequencies

#get the confidence intervals for m
m.ci <- confint(m.fit, 'm', level=0.95)

#get a summary of the model fitting
m.sum <- summary(m.fit) 

#get the estimated coefficient of our model fitting
m.coef = coef(m.fit) 

# Get table of model fit stats
fitstats <- data.frame(m=m.coef, m.low.ci=m.ci[1], m.up.ci=m.ci[2],
                       Rsqr=Rsqr, p.value=m.sum$parameters[4], N=N,
                       Samples=nrow(ASV.table), Richness=length(p.list),
                       Detect=d)

# Get confidence interval for predictions
freq.pred.ci <- binconf(freq.pred*nrow(ASV.table), nrow(ASV.table), alpha=0.05, method="wilson", return.df=TRUE)

# Get table of predictions
pred.df <- data.frame(metacomm_RA=p.list, frequency=freq.pred,
                      frequency_lowerCI=freq.pred.ci[,2],
                      frequency_upperCI=freq.pred.ci[,3]) %>%
  unique()

# Get table of observed occupancy and abundance
obs.df = C.no0 %>%
  rename(metacomm_RA = p, frequency=freq)

#add abundance classifications to obs.df
#obs.df$abun_class <- abun_class$Abundance
abun_class$ASV <- abun_class$ID
new.df <- merge(obs.df, abun_class, by=c("ASV"))

p <- ggplot(data=new.df) +
    geom_point(data=new.df, aes(x=log10(metacomm_RA), y=frequency, color=Abundance),
               alpha=.2, size=2) +
    geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency), color="black") +
    geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency_lowerCI), linetype=2, color="black") +
    geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency_upperCI), linetype=2, color="black") +
    geom_text(data=fitstats, aes(label = paste("R^2 == ", round(Rsqr, 3))),
              x=-4.9, y=0.75, size=4, parse=TRUE) +
    geom_text(data=fitstats, aes(label = paste("italic(m) ==", round(m, 4))),
              x=-4.9, y=0.68, size=4, parse=TRUE) +
    labs(x="Log10 abundance in\nmetacommunity", y="Frequency detected") +
    theme_bw() +
    theme(axis.line = element_line(color="black"),
          #legend.position = "none",
          axis.title = element_text(size=14),
          axis.text = element_text(size=12))+
  ggtitle("16S Neutral Model Plot")

#save plot
    pdf(paste0("../../results/16S/graphs/neutral-model/16S-neutral-model.pdf"))
    print(p)
    dev.off()
}

```

Run model
```{r}
# neutral_model_plot(ps, "Arctic")
# neutral_model_plot(ps, "Antarctic")

neutral_model_plot_2(ps)
```