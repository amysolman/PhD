---
title: "Network Analysis"
author: "Amy Solman"
date: "10/11/2021"
output: html_document
---

This script will perform network analysis on my count data while exploring the differences between rare, abundant and intermediate subcommunities for the Antarctic dataset.

Step One: Clear workspace and load packages
Step Two: Import data
Step Three: Normalise data
Step Four: Classify ASVs by abundance
Step Five: Filter ASVs to simplify network analysis
Step Six: Perform correlation analysis and export correlation analysis as network
Step Seven: Calculate node-level properties (degree, betweenness centrality, closeness centrality, eignevector centrality)
Step Eight: Generate subcommunity networks
Step Nine: Calculate node-level properties
Step Ten: How many connections are there within and between the subcommunities?
Step Eleven: Test for staistical differences between node-level properties of rare, abundant and intermediate subcommunities
Step Twelve: Generate boxplots of node-level properties for subcommunities
Step Thirteen: Identify keystone ASVs
Step Fourteen: Generate random networks
Step Fifteen: Calculate node-level properties for random network
Step Sixteen: Calculate network-level properties for random network
Step Seventeen: Compare the network- and node-level properties of the real and random networks
Step Eighteen: Identify if the real network exhibits scale-free characteristics

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Step One: Clear workspace and load packages
```{r}
rm(list=ls())
graphics.off()

#library(phyloseq)
library(Hmisc)
library(igraph)
#library(agricolae)
library(fdrtool)
library(vegan) #rarecurve function
library(dplyr)
library(ggpubr)
library(car) #levene test
library(plyr)
```

Import 16S and 18S data
```{r}
pro <- readRDS("../../results/16S/phylo-objects/16S-phyloseq-object-rarefied-decontam.rds")
euk <- readRDS("../../results/18S/phylo-objects/18S-phyloseq-object-rarefied.rds")
```

Function to filter ASVs by percentage presence
```{r}
 #Filter ASVs by percentage presence in samples to simplify the network
filter_me <- function(my_df, perc){
  
    #remove ASVs present in less than 10% samples
  df <- 1*(my_df>0) #presence/abundance df
  pres_vec <- vector() 
  asv_vec <- vector()
  for (k in 1:ncol(df)){  #for each ASV
    pres <- as.numeric(colSums(df)[k])/nrow(df) #find the percentage of samples the ASV is present in
    pres_vec <- c(pres_vec, pres)
    asv_vec <- c(asv_vec, colnames(df)[k])
  }
  df_2 <- data.frame(ASV=asv_vec, Present=pres_vec) #df of percentage of samples asvs are present in
  
  trim_me <- subset(df_2, df_2$Present >= perc) #only keep ASVs in >= 10% of samples (that's at least two samples)
  
  #use ASV IDs to subset count table
  asv_pres_trim <- my_df[, (colnames(my_df) %in% trim_me$ASV)]
  
  return(asv_pres_trim)
}  


```

Function to prepare data for analysis
```{r}
#Function testing area
# phylo1 = pro
# phylo2 = euk
# # pole = "Arctic"
# 
# my_ASVs_and_classifications <- network_data_prep(pro, euk)
# #asv table
# my_ASVs_and_classifications$asvs
# #classifications
# my_ASVs_and_classifications$class

network_data_prep <- function(phylo1, phylo2, perc){
  
  #subset by pole
  # ps1.pole <- subset_samples(phylo1, Pole== pole)
  # ps2.pole <- subset_samples(phylo2, Pole== pole)
  ps1.pole <- phylo1
  ps2.pole <- phylo2
  
  #make sure both datasets have the same sample sizes
  #if pro is bigger than euk, cut pro to be same size as euk
  #if pro is smaller than euk, cut euk to be same size as pro
  if (min(sample_sums(ps1.pole)) > min(sample_sums(ps2.pole))){ 
    
      ps1.pole = rarefy_even_depth(ps1.pole, rngseed=1, sample.size=min(sample_sums(ps2.pole)), replace=F)
      
  } else if (min(sample_sums(ps1.pole)) < min(sample_sums(ps2.pole))){
    ps2.pole = rarefy_even_depth(ps2.pole, rngseed=1, sample.size=min(sample_sums(ps1.pole)), replace=F)
  }
  
  
  #transform our count data into relative abundances
  rel_abun1 = transform_sample_counts(ps1.pole, function(x) x / sum(x) )
  rel_abun2 = transform_sample_counts(ps2.pole, function(x) x / sum(x) )
  
  #get our ASV tables
  asv_tab1 <- data.frame(t(otu_table(rel_abun1)), check.names=FALSE)
  asv_tab2 <- data.frame(t(otu_table(rel_abun2)), check.names=FALSE)
  
  #make ASV table row names the sample names
  data1 <- data.frame(sample_data(rel_abun1))
  data2 <- data.frame(sample_data(rel_abun2))
  rownames(asv_tab1) <- data1$Name
  rownames(asv_tab2) <- data2$Name
  
  #only keep samples that are the same across the datasets
  asv_tab1.trim <- asv_tab1[rownames(asv_tab1) %in% rownames(asv_tab2),]
  asv_tab2.trim <- asv_tab2[rownames(asv_tab2) %in% rownames(asv_tab1.trim),]
  
  #order the rows
  new_df1 <- asv_tab1.trim[ order(row.names(asv_tab1.trim)), ]
  new_df2 <- asv_tab2.trim[ order(row.names(asv_tab2.trim)), ]
  
  #seperate ASVs by abundance
  rare1 <- new_df1[,colSums(new_df1)/nrow(new_df1)< 0.00001] 
  abundant1 <- new_df1[,colSums(new_df1)/nrow(new_df1)>0.0005] 
  intermediate1 <- new_df1[,colSums(new_df1)/nrow(new_df1)>=0.00001 & colSums(new_df1)/nrow(new_df1)<=0.0005] 
  
  rare2 <- new_df2[,colSums(new_df2)/nrow(new_df2)< 0.00001] 
  abundant2 <- new_df2[,colSums(new_df2)/nrow(new_df2)>0.0005] 
  intermediate2 <- new_df2[,colSums(new_df2)/nrow(new_df2)>=0.00001 & colSums(new_df2)/nrow(new_df2)<=0.0005]
  
  #create a dataframe with ASV IDs and their abundance classification
  abundance_class_df <- data.frame(ID = c(names(rare1), names(abundant1), names(intermediate1), names(rare2), names(abundant2), names(intermediate2)), Class=c(rep("Rare", ncol(rare1)), rep("Abundant", ncol(abundant1)), rep("Intermediate", ncol(intermediate1)), rep("Rare", ncol(rare2)), rep("Abundant", ncol(abundant2)), rep("Intermediate", ncol(intermediate2))), Kingdom=c(rep("Prokaryote", ncol(rare1)+ncol(abundant1)+ncol(intermediate1)), rep("Eukaryote", ncol(rare2)+ncol(abundant2)+ncol(intermediate2))))  
  
  #filter ASVs to those present in at least % of samples
   pro_filter <- filter_me(new_df1, perc)
   euk_filter <- filter_me(new_df2, perc)
  
   #finally bind out dataframes together
   pro_and_euk <- cbind(pro_filter, euk_filter)
   
   #list for returning data
   newList <- list("class" = abundance_class_df, "asvs" = pro_and_euk)
   
   
   return(newList)
}
```

Function to perform correlation analysis
```{r}
#Function testing area
# phylo1 = pro
# phylo2 = euk
# perc=0.05

get_my_network <- function(phylo1, phylo2, perc){
  
  set.seed(666) 
  
  #get our asv table
  output <- network_data_prep(phylo1, phylo2, perc)
  asv_tab <- output$asvs
  
  #find the spearmans correlations between asvs
  cor_analysis <-rcorr(as.matrix(asv_tab),type="spearman")
  
  #assign the matrix of correlations and the p-values to r and p, respectively
  cor_r<-cor_analysis$r
  cor_p<-cor_analysis$P 
  
  #matrix diagonals - apply the value 1 to the diagonal values of the p matrix (instead of NA as they were before)
  diag(cor_p)<-1
  
  #make p-values matrix into vector
  cor_pp<-as.vector(cor_p)
  
  #how many of our p values are < 0.01
  length(cor_pp[cor_pp > 0 & cor_pp < 0.01]) 
  
  #estimate false discovery rates
  cor_qvalue<-fdrtool(cor_pp,statistic="pvalue")
  
  #extract the vector with q-values (density based falese discovery rates)
  cor_q<-cor_qvalue$qval
  
    #how many of our p values are < 0.01
    length(cor_q[cor_q > 0 & cor_q < 0.01]) 
  
  #what are our unique adjusted p values?
  unique(cor_q) #lots of them! Essentially we don't want to see 1 NaN here (this means we have no significant adjusted p values)
  
  #create a matrix with the q-values with the same number of rows and columns as p-values matrix
  cor_q<-matrix(cor_q,nrow(cor_p),ncol(cor_p))
  
  #make any value of q > 0.01 a 0
  cor_q[cor_q>0.01]<-0
  
  #make any value <= 0.01 & greater than 0 = 1
  cor_q[cor_q<=0.01&cor_q>0]<-1

  #let's look at the range of correlation coefficients
  hist(cor_r)
  
  #are there any with coefficients > | 0.6|?
  length(cor_r[cor_r < -0.6]) #there are 288 negative correlation coefficients
  length(cor_r[cor_r > 0.6]) #there 60567 positive correlation coefficients
  
  #change the value of any correlation coefficients < | 0.6 | to 0
  #cor_r[abs(cor_r)<0.6]<-0
  cor_r[(cor_r)<0.6]<-0
  
  #multiple the correlation coefficients by q-values so that insignificant coefficients become 0
  cor<-cor_r*cor_q
  
  #are there any with coefficients > | 0.6| after removing non-significant correlations?
  length(cor[cor < -0.6]) 
  length(cor[cor > 0.6]) 
  
  #create igraph graph from adjacency matrix
  cor_g <- graph.adjacency(cor, weighted=TRUE, mode="undirected")
  
  #simplify the graph so it does not contain loops or multiple edges
  cor_g <- simplify(cor_g)

  #delete vertices from the graph
  cor_g<-delete.vertices(cor_g,names(degree(cor_g)[degree(cor_g)==0]))
  
  #export graph
  write.graph(cor_g,"../../results/16S/network-analysis/16S-18S-network.gml", format="gml")
  write.graph(cor_g,"../../results/18S/network-analysis/16S-18S-network.gml", format="gml")

  return(cor_g)
}

#Function testing area
#my_graph <- get_my_network(pro, euk, 0.20)

```

Function for calculating node-level properties: degree, betweenness centrality, closeness centrality, eigenvector centrality
```{r}
# #Function testing area
# phylo1 = pro
# phylo2 = euk
# perc=0.05
# 
# node_df <-node_level_properties(pro, euk, 0.05)

node_level_properties <- function(phylo1, phylo2, perc){
  
  output <- network_data_prep(phylo1, phylo2, perc)
  cor_g <- get_my_network(phylo1, phylo2, perc)
  
  #get the info on our classifications
  class_data <- output$class
  rare <- subset(class_data, (class_data$Class == "Rare"))
  abundant <- subset(class_data, (class_data$Class == "Abundant"))
  intermediate <- subset(class_data, (class_data$Class == "Intermediate"))
  
  #create a dataframe with 5 columns and a row for each node (ASV)
  df<-as.data.frame(matrix(NA,ncol=5,nrow=length(degree(cor_g))))

  #make ASV IDs row names of df
  rownames(df)<-names(degree(cor_g))
  
  #name the colums the node-level topological features
  colnames(df)<-c("degree","betweenness","closeness","eigenvector","category")
  
  #categorise the ASVs as rare, abundant or intermediate
  df[intersect(names(degree(cor_g)),rare$ID),5]<-"rare"
  df[intersect(names(degree(cor_g)),abundant$ID),5]<-"abundant"
  df[intersect(names(degree(cor_g)),intermediate$ID),5]<-"intermediate" 
  
  #get betweenness
  btw<-betweenness(cor_g)
  #get closeness centrality
  cls<-closeness(cor_g)
  #get eigenvector centrality
  egv<-evcent(cor_g)
  
  #put the topological features into the dataframe
  df[,1]<-degree(cor_g)
  df[,2]<-btw
  df[,3]<-cls
  df[,4]<-egv$vector
  
  write.csv(df, "../../results/16S/network-analysis/16S-18S-node-level-properties.csv")
  write.csv(df, "../../results/18S/network-analysis/16S-18S-node-level-properties.csv")
  
  return(df)

}
```

Function for getting taxonomy of each node
```{r}

#Function testing area
# phylo1 = pro
# phylo2 = euk
# perc = 0.05

network_tax <- function(phylo1, phylo2, perc){
  
    df <- node_level_properties(phylo1, phylo2, perc)

    #get taxa info of keystone ASVs
    taxonomy1 <- data.frame(tax_table(phylo1))
    taxonomy2 <- data.frame(tax_table(phylo2))
    taxonomy <- rbind(taxonomy1, taxonomy2)
    network_taxa <- subset(taxonomy, rownames(taxonomy) %in% rownames(df))
    
    write.csv(network_taxa, "../../results/16S/network-analysis/16S-18S-taxa.csv")
    write.csv(network_taxa, "../../results/18S/network-analysis/16S-18S-taxa.csv")
    
    return(network_taxa)

}


```

Function for finding network-level properties
```{r}
# #Function testing area
# phylo1 = pro
# phylo2 = euk
# perc=0.05
# 
# net_df <- network_level_properties(pro, euk, 0.05)

network_level_properties <- function(phylo1, phylo2, perc){
  
  cor_g <- get_my_network(phylo1, phylo2, perc)
  df <- node_level_properties(phylo1, phylo2, perc)
  
  #subset the node level dataframe by rare ASVs
  a<-subset(df,category=="rare")
  g_ra<-induced_subgraph(cor_g,rownames(a))
  #abundant
  a<-subset(df,category=="abundant")
  g_abd<-induced_subgraph(cor_g,rownames(a))
  #intermediate
  a<-subset(df,category=="intermediate")
  g_int<-induced_subgraph(cor_g,rownames(a))
  
  #number of nodes (a.k.a. vertices/ASVs)
  gorder(cor_g) #total = 1996
  gorder(g_ra) #rare = 93
  gorder(g_abd) #abundant = 334
  gorder(g_int) #intermediate = 1569
  
  #number of edges
  gsize(cor_g) #total = 57391
  gsize(g_ra) #rare = 122
  gsize(g_abd) #abundant = 4702
  gsize(g_int) #intermediate = 33182
  
  #mean node degree (number of edges), clustering coefficient (probability that the adjacent vertices of a vertex   are connected), average path length, modularity, density, network diameter 
  #create a dataframe to store out network features
  net_df<-as.data.frame(matrix(NA,ncol=8,nrow=4))
  
  net_df[1,]<-c(mean(degree(cor_g)),transitivity(cor_g),average.path.length(cor_g),
  graph.density(cor_g),diameter(cor_g),modularity(walktrap.community(cor_g)), gorder(cor_g), gsize(cor_g))
  
  net_df[2,]<-c(mean(degree(g_abd)),transitivity(g_abd),average.path.length(g_abd),
  graph.density(g_abd),diameter(g_abd),modularity(walktrap.community(g_abd)), gorder(g_abd), gsize(g_abd))
  
  net_df[3,]<-c(mean(degree(g_ra)),transitivity(g_ra),average.path.length(g_ra),
  graph.density(g_ra),diameter(g_ra),modularity(walktrap.community(g_ra)), gorder(g_ra), gsize(g_ra))
  
  net_df[4,]<-c(mean(degree(g_int)),transitivity(g_int),average.path.length(g_int),
  graph.density(g_int),diameter(g_int),modularity(walktrap.community(g_int)), gorder(g_int), gsize(g_int))
  
  colnames(net_df)<-c("AveDegree","ClustCoef","AvePathLen","Density","Diameter","Modularity", "Nodes", "Edges")
  rownames(net_df)<-c("Total", "Abundant","Rare","Intermediate")
  
  write.csv(net_df, "../../results/16S/network-analysis/16S-18S-network-level-properties.csv")
  write.csv(net_df, "../../results/18S/network-analysis/16S-18S-network-level-properties.csv")
  
  return(net_df)
}




```

Function for finding the number of connection between subcommunities
```{r}
# #Function testing area
# phylo1 = pro
# phylo2 = euk
# perc = 0.05
# res <- connections_between_communities(pro, euk, 0.05)

connections_between_communities<- function(phylo1, phylo2, perc){
  
      cor_g <- get_my_network(phylo1, phylo2, perc)
      output <- network_data_prep(phylo1, phylo2, perc)
  
      #get the info on our classifications
      class_data <- output$class
      rare <- subset(class_data, (class_data$Class == "Rare"))
      abundant <- subset(class_data, (class_data$Class == "Abundant"))
      intermediate <- subset(class_data, (class_data$Class == "Intermediate"))
  
       #extract the adjacency matric from the simplified igraph
      mat <- as.data.frame(as_adjacency_matrix(cor_g, sparse = FALSE))


    #make a dataframe with all the pairs of ASVs with significant correlations
    x <- tidyr::gather(mat) #gather data 
    x$ASV_Two <- rep(colnames(mat), nrow(mat)) #add second asv
    x[x==0] <- NA #set 0 to NA
    x2<-x[complete.cases(x),]#remove rows with NA
    x2 <- subset(x2, select = -c(value)) #remove value column
    colnames(x2) <- c("ASV_One", "ASV_Two") #rename columns

    #this leaves us with repeat pairs so we need to remove them
    #remove rows that have the repeated pairs of ASVs
    #create a new dataframe with var 1 as ASV One and var2 as ASV Two
    dat <- data.frame(var1 = x2$ASV_One,var2 = x2$ASV_Two, cor = rep(1, nrow(x2)))
    #remove any rows that have the same two ASVs as another row
    dat1 <- dat[!duplicated(apply(dat,1,function(x) paste(sort(x),collapse=''))),]
    #remove cor column
    dat1 <- dat1[,-c(3)]

    #get abundance of ASV One
    for (i in 1:nrow(dat1)){
      if(dat1$var1[i] %in% rare$ID){
        dat1$var3[i] <- "rare"
      } else if(dat1$var1[i] %in% abundant$ID){
        dat1$var3[i] <- "abundant"
      } else {
        dat1$var3[i] <- "intermediate"
      }
    }
    
    #get abundance of ASV Two
    for (i in 1:nrow(dat1)){
      if(dat1$var2[i] %in% rare$ID){
        dat1$var4[i] <- "rare"
      } else if(dat1$var2[i] %in% abundant$ID){
        dat1$var4[i] <- "abundant"
      } else {
        dat1$var4[i] <- "intermediate"
      }
    }

  #rename the columns
  colnames(dat1) <- c("ASV_One", "ASV_Two", "Abund_One", "Abund_Two")
  
  #table the data
  community_links_df <- data.frame(table(dat1[,3:4]))

  write.csv(community_links_df, "../../results/16S/network-analysis/16S-18S-community-connections.csv")
  write.csv(community_links_df, "../../results/18S/network-analysis/16S-18S-community-connections.csv")
  
}

```

Function for looking at significant differences between subcommunities
```{r}
#Function testing area
phylo1 = pro
phylo2 = euk
perc  = 0.05
# signif_dif_nodes(pro, euk, 0.05)

signif_dif_nodes <- function(phylo1, phylo2, perc){
  
  df <- node_level_properties(phylo1, phylo2, perc)

#Degree
# Perform pairwise comparisons
compare_means(degree ~ category,  data = df)
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("abundant", "intermediate"), c("intermediate", "rare"), c("abundant", "rare") )
p2 <- ggboxplot(df, x = "category", y = "degree",
          color = "category", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y = 300)     # Add global p-value
pdf("../../results/16S/network-analysis/16S-18S-abundance-class-degree-boxplot2.pdf")
print(p2)
dev.off()
pdf("../../results/18S/network-analysis/16S-18S-abundance-class-degree-boxplot2.pdf")
print(p2)
dev.off()

#Between
# Perorm pairwise comparisons
compare_means(betweenness ~ category,  data = df)
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("abundant", "intermediate"), c("intermediate", "rare"), c("abundant", "rare") )
p3 <- ggboxplot(df, x = "category", y = "betweenness",
          color = "category", palette = "jco", outlier.shape = NULL)+
  ylim(0, 5000)+
  stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y = 4000)# Add global p-value
pdf("../../results/16S/network-analysis/16S-18S-abundance-class-betweenness-boxplot.pdf")
print(p3)
dev.off()
pdf("../../results/18S/network-analysis/16S-18S-abundance-class-betweenness-boxplot.pdf")
print(p3)
dev.off()

#Closeness
# Perorm pairwise comparisons
compare_means(closeness ~ category,  data = df)
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("abundant", "intermediate"), c("intermediate", "rare"), c("abundant", "rare") )
p4 <- ggboxplot(df, x = "category", y = "closeness",
          color = "category", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y = 0.0003)# Add global p-value
pdf("../../results/16S/network-analysis/16S-18S-abundance-class-closeness-boxplot.pdf")
print(p4)
dev.off()
pdf("../../results/18S/network-analysis/16S-18S-abundance-class-closeness-boxplot.pdf")
print(p4)
dev.off()

#Eigenvector
# Perform pairwise comparisons
compare_means(eigenvector ~ category,  data = df)
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("abundant", "intermediate"), c("intermediate", "rare"), c("abundant", "rare") )
p5 <- ggboxplot(df, x = "category", y = "eigenvector",
          color = "category", palette = "jco")+ 
  ylim(0, 0.01)+
  stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y=0.005)# Add global p-value
pdf("../../results/16S/network-analysis/16S-18S-abundance-class-eigenvector-boxplot.pdf")
print(p5)
dev.off()
pdf("../../results/18S/network-analysis/16S-18S-abundance-class-eigenvector-boxplot.pdf")
print(p5)
dev.off()

}

```

Function for identifying keystone taxa
```{r}
#Function testing area
# phylo1 = pro
# phylo2 = euk
# perc  = 0.05
# key_tax <- get_my_keystone(pro, euk, 0.05)

get_my_keystone <- function(phylo1, phylo2, perc){
  
  df <- node_level_properties(phylo1, phylo2, perc)
  
  #Keystone species in each co-occurrence network were defined as those with a high degree of ranking in the top 20% among all nodes, and low betweenness centrality, ranking in the bottom 20% among nodes with high degree (Berry & Widder, 2014).

#top 10% degree
keystone_degree <- as.data.frame(df) %>%
        top_frac(n = 0.1, wt = degree)

#botton 10% betweenness
keystone <- as.data.frame(keystone_degree) %>%
        top_frac(n = -0.1, wt = betweenness)

#get taxa info of keystone ASVs
taxonomy1 <- data.frame(tax_table(phylo1))
taxonomy2 <- data.frame(tax_table(phylo2))
taxonomy <- rbind(taxonomy1, taxonomy2)
keystone_taxa <- subset(taxonomy, rownames(taxonomy) %in% rownames(keystone))

write.csv(keystone_taxa, "../../results/16S/network-analysis/16S-18S-keystone-taxa.csv")
write.csv(keystone_taxa, "../../results/18S/network-analysis/16S-18S-keystone-taxa.csv")

return(keystone_taxa)
  
}


```

Function for random network analysis
```{r}
#Function testing area
phylo1 = pro
phylo2 = euk
perc = 0.05 

random_network(pro, euk, 0.05)

random_network <- function(phylo1, phylo2, perc){
  
  cor_g <- get_my_network(phylo1, phylo2, perc)
  net_df <- network_level_properties(phylo1, phylo2, perc)
  
  #generate 1000 random networks with the same number of nodes (asvs) and edges (connections) as our real network 
  set.seed(1000)
  gs <- list()
  for (x in 1:1000) {
    gs[[x]] <- erdos.renyi.game(gorder(cor_g), gsize(cor_g), type = "gnm", directed = FALSE,loops = FALSE)
  }
  
    #mean node degree (number of edges), clustering coefficient (probability that the adjacent vertices of a vertex are connected), average path length, modularity, density, network diameter 
  #create a dataframe to store out network features
  net_df2<-as.data.frame(matrix(NA,ncol=6,nrow=1000))
  colnames(net_df2)<-c("AveDegree","ClustCoef","AvePathLen","Density","Diameter","Modularity")
  for (x in 1:1000){
    net_df2[x,]<-c(mean(degree(gs[[x]])),transitivity(gs[[x]]),average.path.length(gs[[x]]),graph.density(gs[[x]]),diameter(gs[[x]]),modularity(walktrap.community(gs[[x]])))
  }
  
  write.csv(net_df2, "../../results/16S/network-analysis/16S-18S-random-network-results.csv")
  write.csv(net_df2, "../../results/18S/network-analysis/16S-18S-random-network-results.csv")
  
  #compare modularity, clustering coefficient and average path length of 1000 random networks to real network
#boxplot of modularity, clustering coefficient and avergae path length with real network values as lines on graph
#Modularity
ggboxplot(net_df2, y = "Modularity")+
  geom_hline(yintercept=net_df$Modularity[1], linetype="dashed", color = "red")
pdf("../../results/16S/network-analysis/16S-18S-random-network-modularity.pdf")
print(p1)
dev.off()
pdf("../../results/18S/network-analysis/16S-18S-random-network-modularity.pdf")
print(p1)
dev.off()

#Clustering Coefficient
ggboxplot(net_df2, y = "ClustCoef")+
  geom_hline(yintercept=net_df$ClustCoef[1], linetype="dashed", color = "red")
pdf("../../results/16S/network-analysis/16S-18S-random-network-cluster-coefficient.pdf")
print(p2)
dev.off()
pdf("../../results/18S/network-analysis/16S-18S-random-network-cluster-coefficient.pdf")
print(p2)
dev.off()

#Average Path Length
ggboxplot(net_df2, y = "AvePathLen")+
  geom_hline(yintercept=net_df$AvePathLen[1], linetype="dashed", color = "red")
pdf("../../results/16S/network-analysis/16S-18S-random-network-ave-path-len.pdf")
print(p3)
dev.off()
pdf("../../results/18S/network-analysis/16S-18S-random-network-ave-path-len.pdf")
print(p3)
dev.off()

#The real network has higher values for these three topological features than the randomly generated networks, suggesting the network had “small-world” properties and modular structure. 
  
}

```

Function for finding if our network exhibits scale-free characteristics
```{r}

scale_free_characteristics <- function(phylo1, phylo2, perc){
  
  cor_g <- get_my_network(phylo1, phylo2, perc)
  
  #find the number of edges (degrees/connections) between each node (vertex/ASV) in the network
#create a dataframe with 5 columns and a row for each node (ASV)
power_df<-as.data.frame(matrix(NA,ncol=2,nrow=2*length(degree(cor_g))))

#name the colums the node-level topological features
colnames(power_df)<-c("degree","category")

#categorise the as real or random
power_df[,2][1:length(degree(cor_g))] <-"real"
power_df[,2][(length(degree(cor_g))+1):nrow(power_df)] <-"random"

# #find the mean degrees of the random networks
# deg <- as.data.frame(matrix(NA,ncol=1000,nrow=length(degree(cor_g))))
# for (i in 1:1000){
#   deg[,i] <- degree(gs[[i]])
# }

#put the topological features into the dataframe
power_df[,1][1:length(degree(cor_g))]<-degree(cor_g)
power_df[,1][(length(degree(cor_g))+1):nrow(power_df)] <- degree(gs[[1]])
#power_df[,1][(length(degree(cor_g))+1):nrow(power_df)] <- rowMeans(deg)

#plot
real <- subset(power_df, power_df$category == "real")
random <- subset(power_df, power_df$category == "random")
#library(plyr) #overlap with dplyr function which is different for 'count' so need to load plyr after dplyr
real_counts <- plyr::count(real$degree)
random_counts <- plyr::count(random$degree)

plot_df <- as.data.frame(matrix(NA, ncol=3, nrow=nrow(real_counts)+nrow(random_counts)))
colnames(plot_df)<-c("degree", "count", "category")
plot_df[,3][1:nrow(real_counts)] <- rep("real", nrow(real_counts))
plot_df[,3][(nrow(real_counts)+1):nrow(plot_df)] <- rep("random", nrow(random_counts))
plot_df[,2][1:nrow(real_counts)] <- real_counts$freq
plot_df[,2][(nrow(real_counts)+1):nrow(plot_df)] <- random_counts$freq
plot_df[,1][1:nrow(real_counts)] <- real_counts$x
plot_df[,1][(nrow(real_counts)+1):nrow(plot_df)] <- random_counts$x

ggplot(plot_df, aes(x=degree, y=count, color=category)) +
  geom_point()

#Taken from http://chengjun.github.io/web_data_analysis/demo2_simulate_networks/

# plot and fit the power law distribution
fit_power_law = function(graph) {
    # calculate degree
    d = degree(graph, mode = "all")
    dd = degree.distribution(graph, mode = "all", cumulative = FALSE)
    degree = 1:max(d)
    probability = dd[-1]
    # delete blank values
    nonzero.position = which(probability != 0)
    probability = probability[nonzero.position]
    degree = degree[nonzero.position]
    reg = lm(log(probability) ~ log(degree))
    cozf = coef(reg)
    power.law.fit = function(x) exp(cozf[[1]] + cozf[[2]] * log(x))
    alpha = -cozf[[2]]
    R.square = summary(reg)$r.squared
    print(paste("Alpha =", round(alpha, 3)))
    print(paste("R square =", round(R.square, 3)))
    # plot
    plot(probability ~ degree, log = "xy", xlab = "Degree (log)", ylab = "Probability (log)", 
        col = 1, main = "Degree Distribution")
    curve(power.law.fit, col = "red", add = T, n = length(d))
}


scale_plot <- fit_power_law(cor_g) #does appear to be scale-free but not many data points!

pdf("../../results/16S/network-analysis/16S-18S-scale_free_characteristics.pdf")
print(scale_plot)
dev.off()
pdf("../../results/18S/network-analysis/16S-18S-scale_free_characteristics.pdf")
print(scale_plot)
dev.off()

#generate a scale-free network
# g = barabasi.game(100)
# fit_power_law(g) #we can see here that a scale-free network has a much higher R-Squared value 
  
  
}

```
