---
title: "MultiCoLA"
author: "Amy Solman"
date: "19/02/2022"
output: html_document
---
SCRIPTS AND GUIDE DOWNLOADED FROM: https://www.mpi-bremen.de/en/Softwares.html#section1550

Gobet, A., Quince, C., and Ramette, A. 2010. Multivariate Cutoff Level Analysis (MultiCoLA) of Large Community Datasets. Nucl. Acids Res.


*************************************************************************************************************
*Steps of MuliCoLA Analysis*

1) Load and prepare the data

2) Community Structure
2.1) Obtain a matrix for each taxonomic level (when the taxonomic annotation is available only)
2.2) Apply successive cutoffs on each original matrix
2.3) Calculate of Spearman (or Pearson, Kendall) correlations and Procrustes correlations between the original dataset and the truncated ones
2.4) Plot results of cutoff impact

3) Ecological Patterns
3.1) Variation partitioning at several cutoff levels for all taxonomic levels
3.2) Calculation of correlation coefficients for the environmental parameters (for the first RDA axis)
3.3) Calculation of the significance of the whole variation partitioning model and the impact of the pure environmental parameters

4) Save workspace
*************************************************************************************************************

First, set the directory where you want to work, i.e. where your input file and the series of scripts should be, and where you will find the several outputs from these scripts. The directory should be created beforehand and the name should not contain spaces to be readable by the software R (e.g. use underscore to separate words). The path to the working directory (e.g. “454_MPTS”) may be indicated as followed:
setwd("/Users/angeliquegobet/R/454_MPTS")

Clear workspace and import data
```{r}
rm(list=ls())
graphics.off()

#load packages
library(vegan)
library(MASS)

#load phyloseq object
ps_pro <- readRDS("../../results/16S/phylo-objects/16S-phyloseq-object-rarefied-decontam.rds") 
ps_euk <- readRDS("../../results/18S/phylo-objects/18S-phyloseq-object-rarefied.rds") 

#source our data prep function
source("solman-MultiCoLA-input-file-prep.R")

#source pre-written MultiCoLA functions from Gobet et al
source("taxa.pooler.1.4.r") #step 2.1
source("COtables.1.4.r") #step 2.2
source("cutoff.impact.1.4.r") #step 2.3
source("cutoff.impact.fig.1.4.r") #step 2.4 
source("VP.COL.1.4.r") #step 3.1
source("corrcoeff.ENV.1.4.r") #step 3.2
source("signif.1.4.r") #step 3.3
```

16S Arctic Data-set based cut-offs

1) Load and prepare data
I have previously written a function MultiCoLA_input_file_prep() to take a phyloseq object and output data for this analysis.

```{r}
#separate arctic samples
ps_arc <- subset_samples(ps_pro, Pole=="Arctic")
#remove asvs with 0 counts
ps_arc_filter = filter_taxa(ps_arc, function(x) sum(x) >= 1, TRUE)
#prep data
M <- MultiCoLA_input_file_prep(ps_arc_filter)
```

2) Community Structure

2.1) Obtain a matrix for each taxonomic level (when the taxonomic annotation is available only)

```{r}
all_taxa_pooled<-taxa.pooler(M)
```

2.2) Apply successive cutoffs on each original matrix

```{r}
#Remove dominant types 

dom.truncated.DS.phylum<-COtables(all_taxa_pooled[[1]], Type="ADS",typem="dominant")

dom.truncated.DS.class<-COtables(all_taxa_pooled[[2]], Type="ADS",typem="dominant")

dom.truncated.DS.order<-COtables(all_taxa_pooled[[3]], Type="ADS",typem="dominant")

dom.truncated.DS.family<-COtables(all_taxa_pooled[[4]], Type="ADS",typem="dominant")

dom.truncated.DS.genus<-COtables(all_taxa_pooled[[5]], Type="ADS",typem="dominant")

dom.truncated.DS.species<-COtables(all_taxa_pooled[[6]], Type="ADS",typem="dominant")

dom.truncated.DS.OTUcompleteDS<-COtables(all_taxa_pooled[[7]], Type="ADS",typem="dominant")

dom.truncated.DS.OTUwholeDS<-COtables(all_taxa_pooled[[8]], Type="ADS",typem="dominant")

#Remove rare types

rare.truncated.DS.phylum<-COtables(all_taxa_pooled[[1]], Type="ADS",typem="rare")

rare.truncated.DS.class<-COtables(all_taxa_pooled[[2]], Type="ADS",typem="rare")

rare.truncated.DS.order<-COtables(all_taxa_pooled[[3]], Type="ADS",typem="rare")

rare.truncated.DS.family<-COtables(all_taxa_pooled[[4]], Type="ADS",typem="rare")

rare.truncated.DS.genus<-COtables(all_taxa_pooled[[5]], Type="ADS",typem="rare")

rare.truncated.DS.species<-COtables(all_taxa_pooled[[6]], Type="ADS",typem="rare")

rare.truncated.DS.OTUcompleteDS<-COtables(all_taxa_pooled[[7]], Type="ADS",typem="rare")

rare.truncated.DS.OTUwholeDS<-COtables(all_taxa_pooled[[8]], Type="ADS",typem="rare")
```

2.3) Calculate of Spearman (or Pearson, Kendall) correlations and Procrustes correlations between the original dataset and the truncated ones

```{r}
dom.corr.all<-cutoff.impact(all_taxa_pooled,Type="ADS",corcoef="spearman",typem="dominant")

rare.corr.all<-cutoff.impact(all_taxa_pooled,Type="ADS",corcoef="spearman",typem="rare")
```

2.4) Plot results of cutoff impact

With the input, “corr.all” here, as a list (e.g. the output from the cutoff.impact) and you can choose to have the output as a text file:
Output as text files? (y/n)...

Then three files will appear in the directory:
- "abundance.txt"
- "non-par.correlation.txt"
- "procrustes.txt"

And they can be further used to produce figures with Microsoft Excel for example. Or you can also choose if you want to directly plot the data:
Plot the results? (y/n)...

```{r}
output.all<-cutoff.impact.fig(corr.all)
```

3) Ecological Patterns

3.1) Variation partitioning at several cutoff levels for all taxonomic levels

Load the environmental table with samples as rows and environmental parameters as columns.

Some explanations about the function are then appearing.
With:

- The input, “all_taxa_pooled” here, is the output from the taxa.pooler;
- ENV = the environmental table;
- Type = Type of cutoff: all dataset-,”ADS”, or sample-,”SAM”, based.

The output is a list of two tables, for each taxonomic level:
- one with the partition of the variation by the different assigned cutoffs (all dataset- or sample-based);
- one with the different assigned cutoffs by the sum of each truncated table, and the adjusted R square.

You can choose if you want the output as a text file:
Output as text files? (y/n)...

Then two files x the number of taxonomic level will appear in the directory:
- "taxonomiclevel.VarPart.txt"
- "taxonomiclevel.sum.adjRsq.txt" 
And then can be further used to produce figures with Microsoft Excel for example.

Or you can also choose if you want to plot the data:
Plot the results? (y/n)...

If sample-based cutoff chosen, the following question will appear:
If SAM-based only, maximum cutoff value? (e.g. 208)...

```{r}
#get environmental data from phyloseq object
# ENV <- data.frame(sample_data(my_phylo), header=TRUE,row.names=1)
# #remove first 16 columns
# ENV <- ENV[,18:21]

#dummy data
ENV<-read.table("env.txt",header=TRUE,row.names=1)
ENV <- rbind(ENV,ENV,ENV,ENV,ENV)
ENV <- ENV[1:75,]

#Variation partitioning analysis
VP.1.taxa<-VP.COL(all_taxa_pooled,ENV,Type="ADS",typem="dominant")
```

3.2) Calculation of correlation coefficients for the environmental parameters (for the first RDA axis)

However, a whole “automatic” script could not be realized as the R software can present some scoping problems. Instead, you may copy and paste the following lines:

```{r}

#All data set cut offs
#create a matrix to store corrcoeff output at all 21 cutoffs corrcoeff.table.ADS<-matrix(NA,21,5) 
row.names(corrcoeff.table.ADS)<-c(paste("CO_",c(0.01,seq(0.05,0.95,by=0.05),0.99),sep="")) colnames(corrcoeff.table.ADS)<-c("Sum",paste("RDA1.",colnames(ENV),sep=""))

#store the original matrix #7: whole dataset at the OTU level #3: where the cutoff matrices are 
OTU.ADS<-VP.1.taxa[[c(7,3)]]

#application of corrcoeff at all cutoffs SPE<-OTU.ADS[[1]];corrcoeff.table.ADS[1,]<-corrcoeff(SPE,ENV);rm(SPE) 
SPE<-OTU.ADS[[2]];corrcoeff.table.ADS[2,]<-corrcoeff(SPE,ENV);rm(SPE) .
SPE<-OTU.ADS[[21]];corrcoeff.table.ADS[21,]<-corrcoeff(SPE,ENV);rm(SPE)

#application of corrcoeff on the original table with no cutoff SPE<-all_taxa_pooled[[7]] 
corrcoeff.table.ADS.orig<-corrcoeff(SPE,ENV) row.names(corrcoeff.table.ADS.orig)<-c("CO_1") corrcoeff.table.ADS<-rbind(corrcoeff.table.ADS,corrcoeff.table.ADS.orig)

#output as a text file 
write.table(corrcoeff.table.ADS,"corrcoeff.table.ADS.txt",quote=FALSE)

#Sample-based cutoffs
#create a matrix to store corrcoeff output at all 15 cutoffs corrcoeff.table.SAM<-matrix(NA,15,5)  
row.names(corrcoeff.table.SAM)<c(paste("CO_",c(1,2,3,5,10,15,20,30,55,80,105,130,155,180,208),sep="")) colnames(corrcoeff.table.SAM)<-c("Sum",paste("RDA1.",colnames(ENV),sep=""))

#store the original matrix #7: whole dataset at the OTU level #3: where the cutoff matrices are 
OTU.SAM<-VP.1.taxa[[c(7,3)]]

#application of corrcoeff at all cutoffs SPE<-OTU.SAM[[1]];corrcoeff.table.SAM[1,]<-corrcoeff(SPE,ENV);rm(SPE) 
SPE<-OTU.SAM[[2]];corrcoeff.table.SAM[2,]<-corrcoeff(SPE,ENV);rm(SPE) .
SPE<-OTU.SAM[[15]];corrcoeff.table.SAM[15,]<-corrcoeff(SPE,ENV);rm(SPE)

#output as a text file 
write.table(corrcoeff.table.SAM,"corrcoeff.table.SAM.txt",quote=FALSE)
```

3.3) Calculation of the significance of the whole variation partitioning model and the impact of the pure environmental parameters

However, a whole “automatic” script could not be realized as the R software can present some scoping problems. Instead, you may copy and paste the following lines:

```{r}
#Dataset-based cutoffs
#create a matrix to store signif output at all 21 cutoffs signif.table.ADS<-matrix(NA,21,5) 
row.names(signif.table.ADS)<-c(paste("CO_",c(0.01,seq(0.05,0.95,by=0.05),0.99),sep="")) colnames(signif.table.ADS)<- c("whole.sig","ENV1.sig","ENV2.sig","ENV3.sig","ENV4.sig")

#store the original matrix #7: whole dataset at the OTU level #3: where the cutoff matrices are 
OTU.ADS<-VP.1.taxa[[c(7,3)]]

#application of signif at all cutoffs 
SPE<-OTU.ADS[[1]];signif.table.ADS[1,]<-signif(SPE,ENV);rm(SPE) 
SPE<-OTU.ADS[[2]];signif.table.ADS[2,]<-signif(SPE,ENV);rm(SPE)
SPE<-OTU.ADS[[21]];signif.table.ADS[21,]<-signif(SPE,ENV);rm(SPE)

#application of signif on the original table with no cutoff SPE<-all_taxa_pooled[[7]] 
signif.table.ADS.orig<-signif(SPE,ENV) row.names(signif.table.ADS.orig)<-c("CO_1") 
signif.table.ADS<-rbind(signif.table.ADS, signif.table.ADS.orig)

#output as a text file 
write.table(signif.table.ADS,"signif.table.ADS.txt",quote=FALSE)

#Sample-based cutoffs
#create a matrix to store signif output at all 15 cutoffs signif.table.SAM<-matrix(NA,15,5) 
row.names(signif.table.SAM)<c(paste("CO_",c(1,2,3,5,10,15,20,30,55,80,105,130,155,180,208),sep="")) colnames(signif.table.SAM)<- c("whole.sig","ENV1.sig","ENV2.sig","ENV3.sig","ENV4.sig")

#store the original matrix #7: whole dataset at the OTU level #3: where the cutoff matrices are 
OTU.SAM<-VP.1.taxa[[c(7,3)]]

#application of signif at all cutoffs 
SPE<-OTU.SAM[[1]];signif.table.SAM[1,]<-signif(SPE,ENV);rm(SPE) SPE<-OTU.SAM[[2]];signif.table.SAM[2,]<-signif(SPE,ENV);rm(SPE)
SPE<-OTU.SAM[[15]];signif.table.SAM[15,]<-signif(SPE,ENV);rm(SPE)

#output as a text file 
write.table(signif.table.SAM,"signif.table.SAM.txt",quote=FALSE)

```



5. Save your R workspace
All variables will then be saved and then available to work on them without running all the scripts again.
```{r}
save.image("MultiCoLA.RData")
```
