---
title: "Neutral Model"
author: "Amy Solman"
date: "12/10/2021"
output: html_document
---
This script will fit Sloan's Neutral Model to my data.

What is Sloan's Neutral Model?
Sloan's Neutral Model is a mathematical model that explains observed relation abundance (percentage of community a taxa represents) and the frequency with which that taxa is observed (the number of samples it is found in) can be explained by a neutral community model (NCM).

The NCM is a stochastic, birth-death immigration process. IT reproduces the observed species abundance distribution.

If a community are RANDOMLY ASSEMBLED then the frequency with which taxa are observed (the percentage of samples they are found in) should increase as a function of the average abundance of taxa across all samples.

We use it to assess whether the observed composition of communities is consistent with neutral community assembly.

How do you fit a neutral model to your data?
1 Clear workspace and install packages
2 Read in data
1 Get ASV count table
2 Get the mean relative abundance of each ASV across all samples
3 Calculate frequency data for each ASV
4 Put relative abundance and frequency data into dataframe for plotting
5 Fit the neutral model
6 Get ASV frequencies as predicted by stochastic processes
7 Calculate RSquared value for model fitting and model fit stats
8 Get data for plotting
9 Plot the model fit
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Install Packages
```{r}
rm(list=ls())
##Fits the neutral model from Sloan et al. 2006 to an OTU table and returns several fitting statistics. Alternatively, will return predicted occurrence frequencies for each OTU based on their abundance in the metacommunity
#Install the following packages if they haven't been availabled in your computer yet 
library(Hmisc) #binconf function
library(minpack.lm) #for non-linear model fitting
# library(stats4) #mle function
library(dplyr) #for %>% function
library(tibble) #function rownames_to_column
#install.packages("GUniFrac") #for Rarefy funtion
library(GUniFrac)
```

Read in data 
```{r}
#Load data files
#load("../data-output/18S-my-sequence-data.RData") #load all our pre-processed data files

#load phyloseq object
##################################################BOTH POLES#################################################
#ps <- readRDS("../data-output/18S-my-phyloseq-object.rds") #load unrarefied phyloseq object
ps.rar <- readRDS("../data-output/18S-my-phyloseq-object-rarefied.rds") #load rarefied phyloseq objectcol

#All glaciers merged abundances
ps.glaciers.merged.RA <- readRDS("../data-output/18S-all-glaciers-merged-rarefied-abundant.rds") #merged glaciers abundant phyloseq object
ps.glaciers.merged.RR <- readRDS("../data-output/18S-all-glaciers-merged-rarefied-rare.rds") #merged glaciers rare phyloseq object
ps.glaciers.merged.I <- readRDS("../data-output/18S-all-glaciers-merged-rarefied-intermediate.rds") #merged glaciers intermediate phyloseq object

```

1 TOTAL TAXA
1.1 Get count table
```{r}
# Extract the ASV table from the phyloseq object
ASV.table = data.frame(t(otu_table(ps.rar)))

#rename counts columns with class
tax_tab <- data.frame(tax_table(ps.rar))
#replace NAs with Unknown
tax_tab[is.na(tax_tab)] <- "Unknown"
colnames(ASV.table) <- tax_tab$Genus

#aggregate count data
x <- t(ASV.table)
new_df=aggregate(x, by=list(rownames(x)),sum) #aggrediate counts by genus name
new_counts <- t(new_df) #transpose back to ASVs as columns
colnames(new_counts) <- new_counts[1,] #re-add genus names
new_counts <- new_counts[-1,]
df <- data.frame(new_counts)
#make dataframe numeric
df[] <- lapply(df, as.numeric)
ASV.table <- df
#remove unknown ASVs
drop <- c("Unknown")
ASV.table <- ASV.table[ , !(names(ASV.table) %in% drop)]
```

1.2 Get the mean relative abundance of each ASV across all samples
```{r}
#get the mean number of reads per sample (mean sum of each row)
N <- mean(apply(ASV.table, 1, sum))

#get the mean number of reads for each ASV across all samples (mean of each column)
p.m <- apply(ASV.table, 2, mean)

#Remove any zeros
p.m <- p.m[p.m != 0]

#divide the number of mean reads by the total number of reads per sample (mean relative abundance of each ASV globally)
p <- p.m/N
```

1.3 Calculate frequency data for each ASV
```{r}
#make ASV.table into presence/absence table
ASV.table.bi <- 1*(ASV.table>0)

#find the mean of frequence of each column (ASV) e.g. the mean number of samples each ASV is found in
freq.table <- apply(ASV.table.bi, 2, mean) 

#only keep ASVs with a frequency other than 0
freq.table <- freq.table[freq.table != 0] #only keep data that isn't zero
```

1.4 Put relative abundance and frequency data into dataframe for plotting + calculate limit of detection
```{r}
#put the average relative abundance of each taxa into a dataframe
p.df = data.frame(p) %>%
  rownames_to_column(var="ASV") 

#Make into dataframe with ASV name and frequence of occurence (percentage of samples the ASV is present in )
freq.df = data.frame(ASV=names(freq.table), freq=freq.table) 

#Combine dataframes and arrange by relative abundance 
C <- inner_join(p.df,freq.df, by="ASV") %>%
  arrange(p)

# Remove rows with any zero (absent in either source pool or local communities). You already did this, but just to make sure we will do it again.
C.no0 <- C %>%
  filter(freq != 0, p != 0)

#Calculate the limit of detection
d <- 1/N
```

1.5 Fit the neutral model using non-linear-least squares
```{r}
#get vectors of our mean relative abundances and frequencies
p.list <- C.no0$p #this creates a vector of the mean relative abundances of each ASV
freq.list <- C.no0$freq #this creates a list of the mean frequency of each ASV

##Fit model parameter m (immigration rate) using Non-linear least squares (NLS)
m.fit <- nlsLM(freq.list ~ pbeta(d, N*m*p.list, N*m*(1-p.list), lower.tail=FALSE), start=list(m=0.000001)) #using the mean relative abundances of each ASV, the mean frequency of each ASV, the size of the metacommunity and the detection limit, estimate the dispersal rate of the study area using non-linear least squares fitting.
```

1.6 Get ASV frequencies as predicted by stochastic processes
```{r}
freq.pred <- pbeta(d, N*coef(m.fit)*p.list, N*coef(m.fit)*(1-p.list), lower.tail=FALSE) #get the predicted ASV frequencies under stochastic processes

```

1.7 Calculate RSquared value for model fitting and model fit stats
```{r}
#Get R2 value
#how different are the predicted values from our real values
Rsqr <- 1 - (sum((freq.list - freq.pred)^2))/(sum((freq.list - mean(freq.list))^2)) #get the R2 result of observed frequencies against predicted frequencies

#get the confidence intervals for m
m.ci <- confint(m.fit, 'm', level=0.95)

#get a summary of the model fitting
m.sum <- summary(m.fit) 

#get the estimated coefficient of our model fitting
m.coef = coef(m.fit) 

# Get table of model fit stats
fitstats <- data.frame(m=m.coef, m.low.ci=m.ci[1], m.up.ci=m.ci[2],
                       Rsqr=Rsqr, p.value=m.sum$parameters[4], N=N,
                       Samples=nrow(ASV.table), Richness=length(p.list),
                       Detect=d)
``` 

1.8 Get data for plotting
```{r}
# Get confidence interval for predictions
freq.pred.ci <- binconf(freq.pred*nrow(ASV.table), nrow(ASV.table), alpha=0.05, method="wilson", return.df=TRUE)

# Get table of predictions
pred.df <- data.frame(metacomm_RA=p.list, frequency=freq.pred,
                      frequency_lowerCI=freq.pred.ci[,2],
                      frequency_upperCI=freq.pred.ci[,3]) %>%
  unique()

# Get table of observed occupancy and abundance
obs.df = C.no0 %>%
  rename(metacomm_RA = p, frequency=freq)
```

1.9 Plot the model fit
```{r}
global.total.taxa.plot = ggplot(data=obs.df) +
    geom_point(data=obs.df, aes(x=log10(metacomm_RA), y=frequency),
               alpha=.2, size=2, color="grey30") +
    geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency), color="black") +
    geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency_lowerCI), linetype=2, color="black") +
    geom_line(data=pred.df, aes(x=log10(metacomm_RA), y=frequency_upperCI), linetype=2, color="black") +
    geom_text(data=fitstats, aes(label = paste("R^2 == ", round(Rsqr, 3))),
              x=-4.9, y=0.25, size=4, parse=TRUE) +
    geom_text(data=fitstats, aes(label = paste("italic(m) ==", round(m, 4))),
              x=-4.9, y=0.20, size=4, parse=TRUE) +
    labs(x="Log10 abundance in\nmetacommunity", y="Frequency detected") +
    theme_bw() +
    theme(axis.line = element_line(color="black"),
          legend.position = "none",
          axis.title = element_text(size=14),
          axis.text = element_text(size=12))+
  ggtitle("Global: Total Taxa Neutral Model Plot")

global.total.taxa.plot
```

2 ABUNDANT TAXA
2.1 Get count table
```{r}
set.seed(666)
# Extract the ASV table from the phyloseq object
ASV.table.ab = data.frame(t(otu_table(ps.glaciers.merged.RA)))

#rename counts columns with class
tax_tab <- data.frame(tax_table(ps.glaciers.merged.RA))
#replace NAs with Unknown
tax_tab[is.na(tax_tab)] <- "Unknown"
colnames(ASV.table.ab) <- tax_tab$Genus

#aggregate count data
x <- t(ASV.table.ab)
new_df=aggregate(x, by=list(rownames(x)),sum) #aggrediate counts by genus name
new_counts <- t(new_df) #transpose back to ASVs as columns
colnames(new_counts) <- new_counts[1,] #re-add genus names
new_counts <- new_counts[-1,]
df <- data.frame(new_counts)
#make dataframe numeric
df[] <- lapply(df, as.numeric)
ASV.table.ab <- df
#remove unknown ASVs
drop <- c("Unknown")
ASV.table.ab <- ASV.table.ab[ , !(names(ASV.table.ab) %in% drop)]

#only keep samples with over 2000 reads
ASV.table.ab <- ASV.table.ab[rowSums(ASV.table.ab[])>2000,]

#rarey data
ASV.table.ab <- Rarefy(ASV.table.ab, min(rowSums(ASV.table.ab)))$otu.tab.rff

```

2.2 Get the mean relative abundance of each ASV across all samples
```{r}
#get the mean number of reads per sample (mean sum of each row)
n.ab <- mean(apply(ASV.table.ab, 1, sum))

#get the mean number of reads for each ASV across all samples (mean of each column)
p.m.ab <- apply(ASV.table.ab, 2, mean)

#Remove any zeros
p.m.ab <- p.m.ab[p.m.ab != 0]

#divide the number of mean reads by the total number of reads per sample (mean relative abundance of each ASV globally)
p.ab <- p.m.ab/n.ab
```

2.3 Calculate frequency data for each ASV
```{r}
#make ASV.table.ab into presence/absence table
ASV.table.ab.bi <- 1*(ASV.table.ab>0)

#find the mean of frequence of each column (ASV) e.g. the mean number of samples each ASV is found in
freq.table.ab <- apply(ASV.table.ab.bi, 2, mean) 

#only keep ASVs with a frequency other than 0
freq.table.ab <- freq.table.ab[freq.table.ab != 0] #only keep data that isn't zero
```

2.4 Put relative abundance and frequency data into dataframe for plotting + calculate limit of detection
```{r}
#put the average relative abundance of each taxa into a dataframe
p.df = data.frame(p.ab) %>%
  rownames_to_column(var="ASV") 

#Make into dataframe with ASV name and frequence of occurence (percentage of samples the ASV is present in )
freq.df.ab = data.frame(ASV=names(freq.table.ab), freq=freq.table.ab) 

#Combine dataframes and arrange by relative abundance 
C.ab <- inner_join(p.df,freq.df.ab, by="ASV") %>%
  arrange(p.ab)

# Remove rows with any zero (absent in either source pool or local communities). You already did this, but just to make sure we will do it again.
C.no0.ab <- C.ab %>%
  filter(freq != 0, p.ab != 0)

#Calculate the limit of detection
d.ab <- 1/n.ab
```

2.5 Fit the neutral model using non-linear-least squares
```{r}
#get vectors of our mean relative abundances and frequencies
p.list.ab <- C.no0.ab$p.ab #this creates a vector of the mean relative abundances of each ASV
freq.list.ab <- C.no0.ab$freq #this creates a list of the mean frequency of each ASV

##Fit model parameter m (immigration rate) using non-linear least squares (nLS)
m.fit.ab <- nlsLM(freq.list.ab ~ pbeta(d.ab, n.ab*m*p.list.ab, n.ab*m*(1-p.list.ab), lower.tail=FALSE), start=list(m=0.000001)) #using the mean relative abundances of each ASV, the mean frequency of each ASV, the size of the metacommunity and the detection limit, estimate the dispersal rate of the study area using non-linear least squares fitting.
```

2.6 Get ASV frequencies as predicted by stochastic processes
```{r}
freq.pred.ab <- pbeta(d.ab, n.ab*coef(m.fit.ab)*p.list.ab, n.ab*coef(m.fit.ab)*(1-p.list.ab), lower.tail=FALSE) #get the predicted ASV frequencies under stochastic processes

```

2.7 Calculate RSquared value for model fitting and model fit stats
```{r}
#Get R2 value
Rsqr.ab <- 1 - (sum((freq.list.ab - freq.pred.ab)^2))/(sum((freq.list.ab - mean(freq.list.ab))^2)) #get the R2 result of observed frequencies against predicted frequencies

#get the confidence intervals for m
m.ci.ab <- confint(m.fit.ab, 'm', level=0.95)

#get a summary of the model fitting
m.sum.ab <- summary(m.fit.ab) 

#get the estimated coefficient of our model fitting
m.coef.ab = coef(m.fit.ab) 

# Get table of model fit stats
fitstats.ab <- data.frame(m=m.coef.ab, m.low.ci=m.ci.ab[1], m.up.ci=m.ci.ab[2],
                       Rsqr.ab=Rsqr.ab, p.value=m.sum.ab$parameters[4], n=n.ab,
                       Samples=nrow(ASV.table.ab), Richness=length(p.list.ab),
                       Detect=d.ab)
``` 

2.8 Get data for plotting
```{r}
# Get confidence interval for predictions
freq.pred.ab.ci <- binconf(freq.pred.ab*nrow(ASV.table.ab), nrow(ASV.table.ab), alpha=0.05, method="wilson", return.df=TRUE)

# Get table of predictions
pred.df.ab <- data.frame(metacomm_RA=p.list.ab, frequency=freq.pred.ab,
                      frequency_lowerCI=freq.pred.ab.ci[,2],
                      frequency_upperCI=freq.pred.ab.ci[,3]) %>%
  unique()

# Get table of observed occupancy and abundance
obs.df.ab = C.no0.ab %>%
  rename(metacomm_RA = p.ab, frequency=freq)
```

2.8 Plot the model fit
```{r}
global.abundant.taxa.plot = ggplot(data=obs.df.ab) +
    geom_point(data=obs.df.ab, aes(x=log10(metacomm_RA), y=frequency),
               alpha=.2, size=2, color="grey30") +
    geom_line(data=pred.df.ab, aes(x=log10(metacomm_RA), y=frequency), color="black") +
    geom_line(data=pred.df.ab, aes(x=log10(metacomm_RA), y=frequency_lowerCI), linetype=2, color="black") +
    geom_line(data=pred.df.ab, aes(x=log10(metacomm_RA), y=frequency_upperCI), linetype=2, color="black") +
    geom_text(data=fitstats.ab, aes(label = paste("R^2 == ", round(Rsqr.ab, 3))),
              x=-3, y=0.4, size=4, parse=TRUE) +
    geom_text(data=fitstats.ab, aes(label = paste("italic(m) ==", round(m, 4))),
              x=-3, y=0.3, size=4, parse=TRUE) +
    labs(x="Log10 abundance in\nmetacommunity", y="Frequency detected") +
    theme_bw() +
    theme(axis.line = element_line(color="black"),
          legend.position = "none",
          axis.title = element_text(size=14),
          axis.text = element_text(size=12))+
  ggtitle("Global: Abundant Taxa Neutral Model Plot")

global.abundant.taxa.plot
```

3 INTERMEDIATE TAXA
3.1 Get count table
```{r}
set.seed(666)
# Extract the ASV table from the phyloseq object
ASV.table.i = data.frame(t(otu_table(ps.glaciers.merged.I)))

#rename counts columns with class
tax_tab <- data.frame(tax_table(ps.glaciers.merged.I))
#replace NAs with Unknown
tax_tab[is.na(tax_tab)] <- "Unknown"
colnames(ASV.table.i) <- tax_tab$Genus

#aggregate count data
x <- t(ASV.table.i)
new_df=aggregate(x, by=list(rownames(x)),sum) #aggrediate counts by genus name
new_counts <- t(new_df) #transpose back to ASVs as columns
colnames(new_counts) <- new_counts[1,] #re-add genus names
new_counts <- new_counts[-1,]
df <- data.frame(new_counts)
#make dataframe numeric
df[] <- lapply(df, as.numeric)
ASV.table.i <- df
#remove unknown ASVs
drop <- c("Unknown")
ASV.table.i <- ASV.table.i[ , !(names(ASV.table.i) %in% drop)]

#only keep samples with over 2000 reads
ASV.table.i <- ASV.table.i[rowSums(ASV.table.i[])>200,]

#rarey data
ASV.table.i <- Rarefy(ASV.table.i, min(rowSums(ASV.table.i)))$otu.tab.rff
```

3.2 Get the mean relative abundance of each ASV across all samples
```{r}
#get the mean number of reads per sample (mean sum of each row)
n.i <- mean(apply(ASV.table.i, 1, sum))

#get the mean number of reads for each ASV across all samples (mean of each column)
p.m.i <- apply(ASV.table.i, 2, mean)

#Remove any zeros
p.m.i <- p.m.i[p.m.i != 0]

#divide the number of mean reads by the total number of reads per sample (mean relative abundance of each ASV globally)
p.i <- p.m.i/n.i
```

3.3 Calculate frequency data for each ASV
```{r}
#make ASV.table.i into presence/absence table
ASV.table.i.bi <- 1*(ASV.table.i>0)

#find the mean of frequence of each column (ASV) e.g. the mean number of samples each ASV is found in
freq.table.i <- apply(ASV.table.i.bi, 2, mean) 

#only keep ASVs with a frequency other than 0
freq.table.i <- freq.table.i[freq.table.i != 0] #only keep data that isn't zero
```

3.4 Put relative abundance and frequency data into dataframe for plotting + calculate limit of detection
```{r}
#put the average relative abundance of each taxa into a dataframe
p.df.i = data.frame(p.i) %>%
  rownames_to_column(var="ASV") 

#Make into dataframe with ASV name and frequence of occurence (percentage of samples the ASV is present in )
freq.df.i = data.frame(ASV=names(freq.table.i), freq=freq.table.i) 

#Combine dataframes and arrange by relative abundance 
C.i <- inner_join(p.df.i,freq.df.i, by="ASV") %>%
  arrange(p.i)

# Remove rows with any zero (absent in either source pool or local communities). You already did this, but just to make sure we will do it again.
C.no0.i <- C.i %>%
  filter(freq != 0, p.i != 0)

#Calculate the limit of detection
d.i <- 1/n.i
```

3.5 Fit the neutral model using non-linear-least squares
```{r}
#get vectors of our mean relative abundances and frequencies
p.list.i <- C.no0.i$p.i #this creates a vector of the mean relative abundances of each ASV
freq.list.i <- C.no0.i$freq #this creates a list of the mean frequency of each ASV

##Fit model parameter m (immigration rate) using non-linear least squares (nLS)
m.fit.i <- nlsLM(freq.list.i ~ pbeta(d.i, n.i*m*p.list.i, n.i*m*(1-p.list.i), lower.tail=FALSE), start=list(m=0.000001)) #using the mean relative abundances of each ASV, the mean frequency of each ASV, the size of the metacommunity and the detection limit, estimate the dispersal rate of the study area using non-linear least squares fitting.
```

3.6 Get ASV frequencies as predicted by stochastic processes
```{r}
freq.pred.i <- pbeta(d.i, n.i*coef(m.fit.i)*p.list.i, n.i*coef(m.fit.i)*(1-p.list.i), lower.tail=FALSE) #get the predicted ASV frequencies under stochastic processes

```

3.7 Calculate RSquared value for model fitting and model fit stats
```{r}
#Get R2 value
Rsqr.i <- 1 - (sum((freq.list.i - freq.pred.i)^2))/(sum((freq.list.i - mean(freq.list.i))^2)) #get the R2 result of observed frequencies against predicted frequencies

#get the confidence intervals for m
m.ci.i <- confint(m.fit.i, 'm', level=0.95)

#get a summary of the model fitting
m.sum.i <- summary(m.fit.i) 

#get the estimated coefficient of our model fitting
m.coef.i = coef(m.fit.i) 

# Get table of model fit stats
fitstats.i <- data.frame(m=m.coef.i, m.low.ci=m.ci.i[1], m.up.ci=m.ci.i[2],
                       Rsqr.i=Rsqr.i, p.value=m.sum.i$parameters[4], n=n.i,
                       Samples=nrow(ASV.table.i), Richness=length(p.list.i),
                       Detect=d.i)
``` 

3.8 Get data for plotting
```{r}
# Get confidence interval for predictions
freq.pred.i.ci <- binconf(freq.pred.i*nrow(ASV.table.i), nrow(ASV.table.i), alpha=0.05, method="wilson", return.df=TRUE)

# Get table of predictions
pred.df.i <- data.frame(metacomm_RA=p.list.i, frequency=freq.pred.i,
                      frequency_lowerCI=freq.pred.i.ci[,2],
                      frequency_upperCI=freq.pred.i.ci[,3]) %>%
  unique()

# Get table of observed occupancy and abundance
obs.df.i = C.no0.i %>%
  rename(metacomm_RA = p.i, frequency=freq)
```

3.8 Plot the model fit
```{r}
global.intermediate.taxa.plot = ggplot(data=obs.df.i) +
    geom_point(data=obs.df.i, aes(x=log10(metacomm_RA), y=frequency),
               alpha=.2, size=2, color="grey30") +
    geom_line(data=pred.df.i, aes(x=log10(metacomm_RA), y=frequency), color="black") +
    geom_line(data=pred.df.i, aes(x=log10(metacomm_RA), y=frequency_lowerCI), linetype=2, color="black") +
    geom_line(data=pred.df.i, aes(x=log10(metacomm_RA), y=frequency_upperCI), linetype=2, color="black") +
    geom_text(data=fitstats.i, aes(label = paste("R^2 == ", round(Rsqr.i, 3))),
              x=-3.5, y=0.4, size=4, parse=TRUE) +
    geom_text(data=fitstats.i, aes(label = paste("italic(m) ==", round(m, 4))),
              x=-3.5, y=0.35, size=4, parse=TRUE) +
    labs(x="Log10 abundance in\nmetacommunity", y="Frequency detected") +
    theme_bw() +
    theme(axis.line = element_line(color="black"),
          legend.position = "none",
          axis.title = element_text(size=14),
          axis.text = element_text(size=12))+
  ggtitle("Global: Intermediate Taxa Neutral Model Plot")

global.intermediate.taxa.plot
```

4 RARE TAXA
4.1 Get count table
```{r}
set.seed(666)
# Extract the ASV table from the phyloseq object
ASV.table.ra = data.frame(t(otu_table(ps.glaciers.merged.RA)))

#rename counts columns with class
tax_tab <- data.frame(tax_table(ps.glaciers.merged.RA))
#replace NAs with Unknown
tax_tab[is.na(tax_tab)] <- "Unknown"
colnames(ASV.table.ra) <- tax_tab$Genus

#aggregate count data
x <- t(ASV.table.ra)
new_df=aggregate(x, by=list(rownames(x)),sum) #aggrediate counts by genus name
new_counts <- t(new_df) #transpose back to ASVs as columns
colnames(new_counts) <- new_counts[1,] #re-add genus names
new_counts <- new_counts[-1,]
df <- data.frame(new_counts)
#make dataframe numeric
df[] <- lapply(df, as.numeric)
ASV.table.ra <- df
#remove unknown ASVs
drop <- c("Unknown")
ASV.table.ra <- ASV.table.ra[ , !(names(ASV.table.ra) %in% drop)]

#only keep samples with over 2000 reads
ASV.table.ra <- ASV.table.ra[rowSums(ASV.table.ra[])>3000,]

#rarey data
ASV.table.ra <- Rarefy(ASV.table.ra, min(rowSums(ASV.table.ra)))$otu.tab.rff
```

4.2 Get the mean relative abundance of each ASV across all samples
```{r}
#get the mean number of reads per sample (mean sum of each row)
n.ra <- mean(apply(ASV.table.ra, 1, sum))

#get the mean number of reads for each ASV across all samples (mean of each column)
p.m.ra <- apply(ASV.table.ra, 2, mean)

#Remove any zeros
p.m.ra <- p.m.ra[p.m.ra != 0]

#divide the number of mean reads by the total number of reads per sample (mean relative abundance of each ASV globally)
p.ra <- p.m.ra/n.ra
```

4.3 Calculate frequency data for each ASV
```{r}
#make ASV.table.ra into presence/absence table
ASV.table.ra.bi <- 1*(ASV.table.ra>0)

#find the mean of frequence of each column (ASV) e.g. the mean number of samples each ASV is found in
freq.table.ra <- apply(ASV.table.ra.bi, 2, mean) 

#only keep ASVs with a frequency other than 0
freq.table.ra <- freq.table.ra[freq.table.ra != 0] #only keep data that isn't zero
```

4.4 Put relative abundance and frequency data into dataframe for plotting + calculate limit of detection
```{r}
#put the average relative abundance of each taxa into a dataframe
p.df.ra = data.frame(p.ra) %>%
  rownames_to_column(var="ASV") 

#Make into dataframe with ASV name and frequence of occurence (percentage of samples the ASV is present in )
freq.df.ra = data.frame(ASV=names(freq.table.ra), freq=freq.table.ra) 

#Combine dataframes and arrange by relative abundance 
C.ra <- inner_join(p.df.ra,freq.df.ra, by="ASV") %>%
  arrange(p.ra)

# Remove rows with any zero (absent in either source pool or local communities). You already did this, but just to make sure we will do it again.
C.no0.ra <- C.ra %>%
  filter(freq != 0, p.ra != 0)

#Calculate the limit of detection
d.ra <- 1/n.ra
```

4.5 Fit the neutral model using non-linear-least squares
```{r}
#get vectors of our mean relative abundances and frequencies
p.list.ra <- C.no0.ra$p.ra #this creates a vector of the mean relative abundances of each ASV
freq.list.ra <- C.no0.ra$freq #this creates a list of the mean frequency of each ASV

##Fit model parameter m (immigration rate) using non-linear least squares (nLS)
m.fit.ra <- nlsLM(freq.list.ra ~ pbeta(d.ra, n.ra*m*p.list.ra, n.ra*m*(1-p.list.ra), lower.tail=FALSE), start=list(m=0.000001)) #using the mean relative abundances of each ASV, the mean frequency of each ASV, the size of the metacommunity and the detection limit, estimate the dispersal rate of the study area using non-linear least squares fitting.
```

4.6 Get ASV frequencies as predicted by stochastic processes
```{r}
freq.pred.ra <- pbeta(d.ra, n.ra*coef(m.fit.ra)*p.list.ra, n.ra*coef(m.fit.ra)*(1-p.list.ra), lower.tail=FALSE) #get the predicted ASV frequencies under stochastic processes

```

4.7 Calculate RSquared value for model fitting and model fit stats
```{r}
#Get R2 value
Rsqr.ra <- 1 - (sum((freq.list.ra - freq.pred.ra)^2))/(sum((freq.list.ra - mean(freq.list.ra))^2)) #get the R2 result of observed frequencies against predicted frequencies

#get the confidence intervals for m
#m.ci.ra <- confint(m.fit.ra, 'm', level=0.95)
m.ci.ra <- c(NA, NA) #didn't fit properly

#get a summary of the model fitting
m.sum.ra <- summary(m.fit.ra) 

#get the estimated coefficient of our model fitting
m.coef.ra = coef(m.fit.ra) 

# Get table of model fit stats
fitstats.ra <- data.frame(m=m.coef.ra, m.low.ci=m.ci.ra[1], m.up.ci=m.ci.ra[2],
                       Rsqr.ra=Rsqr.ra, p.value=m.sum.ra$parameters[4], n=n.ra,
                       Samples=nrow(ASV.table.ra), Richness=length(p.list.ra),
                       Detect=d.ra)
``` 

4.8 Get data for plotting
```{r}
# Get confidence interval for predictions
freq.pred.ra.ci <- binconf(freq.pred.ra*nrow(ASV.table.ra), nrow(ASV.table.ra), alpha=0.05, method="wilson", return.df=TRUE)

# Get table of predictions
pred.df.ra <- data.frame(metacomm_RA=p.list.ra, frequency=freq.pred.ra,
                      frequency_lowerCI=freq.pred.ra.ci[,2],
                      frequency_upperCI=freq.pred.ra.ci[,3]) %>%
  unique()

# Get table of observed occupancy and abundance
obs.df.ra = C.no0.ra %>%
  rename(metacomm_RA = p.ra, frequency=freq)
```

4.8 Plot the model fit
```{r}
global.rare.taxa.plot = ggplot(data=obs.df.ra) +
    geom_point(data=obs.df.ra, aes(x=log10(metacomm_RA), y=frequency),
               alpha=.2, size=2, color="grey30") +
    geom_line(data=pred.df.ra, aes(x=log10(metacomm_RA), y=frequency), color="black") +
    geom_line(data=pred.df.ra, aes(x=log10(metacomm_RA), y=frequency_lowerCI), linetype=2, color="black") +
    geom_line(data=pred.df.ra, aes(x=log10(metacomm_RA), y=frequency_upperCI), linetype=2, color="black") +
    geom_text(data=fitstats.ra, aes(label = paste("R^2 == ", round(Rsqr.ra, 3))),
              x=-3, y=0.4, size=4, parse=TRUE) +
    geom_text(data=fitstats.ra, aes(label = paste("italic(m) ==", round(m, 4))),
              x=-3, y=0.35, size=4, parse=TRUE) +
    labs(x="Log10 abundance in\nmetacommunity", y="Frequency detected") +
    theme_bw() +
    theme(axis.line = element_line(color="black"),
          legend.position = "none",
          axis.title = element_text(size=14),
          axis.text = element_text(size=12))+
  ggtitle("Global: Rare Taxa Neutral Model Plot")

global.rare.taxa.plot
```